[â¬…ï¸ Voltar](../README.md)

# MÃ³dulo 6: Modelos de SequÃªncia

## ğŸ¯ Objetivos do MÃ³dulo

Ao final deste mÃ³dulo, vocÃª serÃ¡ capaz de:
- Implementar modelos de n-gramas para linguagem
- Compreender e aplicar Hidden Markov Models (HMM)
- Utilizar Conditional Random Fields (CRF)
- Aplicar modelos sequenciais em POS tagging
- Implementar Named Entity Recognition (NER)
- Avaliar modelos de sequÃªncia adequadamente

## ğŸ“š ConteÃºdo TeÃ³rico

### 1. Modelos de N-gramas

#### 1.1 Conceito BÃ¡sico
Modelos de linguagem que predizem prÃ³xima palavra baseado em n-1 palavras anteriores.

**Tipos:**
- **Unigrama**: P(wi) - independÃªncia total
- **Bigrama**: P(wi|wi-1) - depende da palavra anterior
- **Trigrama**: P(wi|wi-1,wi-2) - depende das duas anteriores

#### 1.2 EstimaÃ§Ã£o de Probabilidades
P(wi|wi-n+1...wi-1) = count(wi-n+1...wi) / count(wi-n+1...wi-1)

**Problemas:**
- Sparse data: combinaÃ§Ãµes nÃ£o vistas
- Zero probabilities: palavras OOV
- Data sparsity aumenta com n

#### 1.3 SuavizaÃ§Ã£o (Smoothing)
- **Add-one (Laplace)**: Adiciona 1 a todas contagens
- **Add-k**: Adiciona k < 1
- **Good-Turing**: Baseado em frequÃªncias de frequÃªncias
- **Kneser-Ney**: Estado da arte para n-gramas

### 2. Hidden Markov Models (HMM)

#### 2.1 Componentes
- **Estados ocultos**: O que queremos inferir
- **ObservaÃ§Ãµes**: O que vemos (palavras)
- **Probabilidades de transiÃ§Ã£o**: P(si|si-1)
- **Probabilidades de emissÃ£o**: P(wi|si)

#### 2.2 Algoritmos Fundamentais

**Forward Algorithm:**
- Calcula probabilidade de sequÃªncia
- Dynamic programming
- O(TÃ—NÂ²) complexidade

**Viterbi Algorithm:**
- Encontra sequÃªncia mais provÃ¡vel de estados
- DecodificaÃ§Ã£o MAP
- Usado em POS tagging

**Baum-Welch Algorithm:**
- Treinamento nÃ£o supervisionado
- EM algorithm para HMMs
- Estima parÃ¢metros dos dados

#### 2.3 AplicaÃ§Ãµes em NLP
- **POS Tagging**: Estados = tags gramaticais
- **Speech Recognition**: Estados = fonemas
- **Gene Prediction**: Estados = regiÃµes genÃ©ticas

### 3. Conditional Random Fields (CRF)

#### 3.1 MotivaÃ§Ã£o
- HMMs fazem assumption de independÃªncia forte
- CRFs modelam P(y|x) diretamente
- Permitem features arbitrÃ¡rias
- Treinamento discriminativo

#### 3.2 FormulaÃ§Ã£o MatemÃ¡tica
P(y|x) = (1/Z(x)) Ã— exp(âˆ‘Î»ifi(yi-1,yi,x,i))

**Componentes:**
- **Feature functions**: fi(yi-1,yi,x,i)
- **Weights**: Î»i
- **Partition function**: Z(x)

#### 3.3 Features em CRFs
- **Transition features**: Dependem de yi-1, yi
- **Observation features**: Dependem de yi, xi
- **Custom features**: Regex, gazeteers, etc.

### 4. Part-of-Speech (POS) Tagging

#### 4.1 DefiniÃ§Ã£o da Tarefa
Atribuir categoria gramatical a cada palavra.

**Tags comuns:**
- **NN**: Noun (substantivo)
- **VB**: Verb (verbo)
- **JJ**: Adjective (adjetivo)
- **RB**: Adverb (advÃ©rbio)
- **DT**: Determiner (determinante)

#### 4.2 Desafios
- **Ambiguidade**: "bank" = substantivo ou verbo
- **Unknown words**: Palavras nÃ£o vistas
- **Context dependency**: "running water" vs "running fast"

#### 4.3 Abordagens
- **Rule-based**: Regras linguÃ­sticas
- **Stochastic**: HMM, CRF
- **Neural**: RNN, LSTM, Transformer

### 5. Named Entity Recognition (NER)

#### 5.1 Tipos de Entidades
- **PERSON**: Nomes de pessoas
- **LOCATION**: Lugares geogrÃ¡ficos
- **ORGANIZATION**: Empresas, instituiÃ§Ãµes
- **MISCELLANEOUS**: Outros (eventos, produtos)

#### 5.2 Esquemas de AnotaÃ§Ã£o

**IOB (Inside-Outside-Begin):**
- B-PER: InÃ­cio de pessoa
- I-PER: ContinuaÃ§Ã£o de pessoa
- O: Fora de entidade

**BILOU:**
- B: Begin, I: Inside, L: Last, O: Outside, U: Unit

#### 5.3 Features para NER
- **Lexical**: Palavra atual, prefixos, sufixos
- **Orthographic**: MaiÃºsculas, pontuaÃ§Ã£o, nÃºmeros
- **Contextual**: Palavras vizinhas
- **Gazetteers**: Listas de nomes conhecidos

## ğŸ›  ImplementaÃ§Ãµes PrÃ¡ticas

### N-grama com NLTK
```python
from nltk.lm import MLE
from nltk.lm.preprocessing import padded_everygram_pipeline

# Preparar dados
train, vocab = padded_everygram_pipeline(3, corpus)

# Treinar modelo
lm = MLE(3)
lm.fit(train, vocab)

# Usar modelo
prob = lm.score("word", ["previous", "words"])
```

### HMM com NLTK
```python
from nltk.tag import hmm

# Treinar tagger
trainer = hmm.HiddenMarkovModelTrainer()
tagger = trainer.train_supervised(training_data)

# Usar tagger
tags = tagger.tag(["The", "cat", "sat"])
```

### CRF com sklearn-crfsuite
```python
import sklearn_crfsuite

# Definir features
def word2features(sent, i):
    word = sent[i][0]
    features = {
        'word.lower()': word.lower(),
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
    }
    return features

# Treinar CRF
crf = sklearn_crfsuite.CRF()
crf.fit(X_train, y_train)
```

## ğŸ“Š ExercÃ­cios PrÃ¡ticos

### ExercÃ­cio 1: Modelo de Linguagem N-grama
- Implementar e comparar diferentes ordens
- Avaliar perplexidade
- Implementar suavizaÃ§Ã£o

### ExercÃ­cio 2: POS Tagging com HMM
- Treinar tagger em corpus anotado
- Avaliar accuracy por tag
- Analisar erros comuns

### ExercÃ­cio 3: NER com CRF
- Feature engineering para entidades
- Comparar diferentes esquemas de anotaÃ§Ã£o
- Avaliar precision, recall, F1

### ExercÃ­cio 4: AnÃ¡lise Comparativa
- HMM vs CRF para POS tagging
- Diferentes features para NER
- Error analysis detalhada

## ğŸ¯ PrÃ³ximos Passos

No **MÃ³dulo 7**, mergulharemos em deep learning para NLP com RNNs, LSTMs e attention.

---

**Dica**: Execute o notebook `06_modelos_sequencia.ipynb` para implementar todos os modelos!

[â¬…ï¸ Voltar](../README.md) 