# M√≥dulo 3: An√°lise Estat√≠stica de Texto

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Realizar an√°lise de frequ√™ncia de palavras e caracteres
- Implementar e interpretar n-gramas
- Calcular medidas de similaridade entre textos
- Aplicar an√°lise de sentimentos b√°sica
- Criar visualiza√ß√µes eficazes para dados textuais
- Detectar padr√µes e tend√™ncias em corpus textuais

## üìö Conte√∫do Te√≥rico

### 1. An√°lise de Frequ√™ncia

#### 1.1 Frequ√™ncia de Palavras
A an√°lise de frequ√™ncia √© fundamental para entender a distribui√ß√£o de palavras em um corpus.

**Lei de Zipf**: A frequ√™ncia de uma palavra √© inversamente proporcional ao seu ranking.
- 1¬™ palavra mais frequente aparece ~2x mais que a 2¬™
- 2¬™ palavra aparece ~2x mais que a 4¬™
- Padr√£o: f(r) ‚àù 1/r

**M√©tricas importantes:**
- **Frequ√™ncia absoluta**: Contagem bruta de ocorr√™ncias
- **Frequ√™ncia relativa**: Propor√ß√£o em rela√ß√£o ao total
- **Vocabul√°rio**: N√∫mero de palavras √∫nicas
- **Hapax legomena**: Palavras que aparecem apenas uma vez

#### 1.2 Distribui√ß√µes Estat√≠sticas
- **Distribui√ß√£o de Zipf**: Para frequ√™ncia de palavras
- **Distribui√ß√£o de Poisson**: Para eventos raros
- **Distribui√ß√£o normal**: Para caracter√≠sticas cont√≠nuas

### 2. N-gramas e Coloca√ß√µes

#### 2.1 N-gramas
Sequ√™ncias cont√≠guas de n palavras que capturam contexto local.

**Tipos de n-gramas:**
- **Unigramas** (n=1): Palavras individuais
- **Bigramas** (n=2): Pares de palavras consecutivas
- **Trigramas** (n=3): Triplas de palavras
- **4-gramas** e superiores: Contextos mais longos

**Aplica√ß√µes:**
- Detec√ß√£o de frases idiom√°ticas
- An√°lise de estilo de escrita
- Modelos de linguagem
- Corre√ß√£o ortogr√°fica

#### 2.2 Coloca√ß√µes
Combina√ß√µes de palavras que aparecem juntas com frequ√™ncia acima do esperado.

**Medidas de associa√ß√£o:**
- **PMI (Pointwise Mutual Information)**
- **Chi-quadrado (œá¬≤)**
- **T-score**
- **Log-likelihood ratio**

### 3. Medidas de Similaridade

#### 3.1 Similaridade Baseada em Contagem
- **Jaccard**: |A ‚à© B| / |A ‚à™ B|
- **Dice**: 2|A ‚à© B| / (|A| + |B|)
- **Overlap**: |A ‚à© B| / min(|A|, |B|)

#### 3.2 Similaridade Baseada em Vetores
- **Cosine similarity**: cos(Œ∏) = A¬∑B / (||A|| ||B||)
- **Dist√¢ncia euclidiana**: ||A - B||
- **Dist√¢ncia de Manhattan**: Œ£|ai - bi|

#### 3.3 Similaridade de Strings
- **Dist√¢ncia de Levenshtein**: Edi√ß√µes m√≠nimas
- **Jaro-Winkler**: Para nomes pr√≥prios
- **Soundex**: Similaridade fon√©tica

### 4. An√°lise de Sentimentos

#### 4.1 Abordagens Lexicais
Baseadas em dicion√°rios de palavras com polaridade.

**Lexicons populares:**
- **VADER**: Valence Aware Dictionary and sEntiment Reasoner
- **SentiWordNet**: WordNet com scores de sentimento
- **AFINN**: Lista de palavras com scores -5 a +5
- **TextBlob**: Implementa√ß√£o simples

#### 4.2 Caracter√≠sticas dos Sentimentos
- **Polaridade**: Positivo, negativo, neutro
- **Intensidade**: Grau do sentimento (0.0 a 1.0)
- **Subjetividade**: Objetivo vs subjetivo
- **Emo√ß√µes**: Alegria, raiva, medo, tristeza, etc.

#### 4.3 Desafios
- **Ironia e sarcasmo**
- **Contexto dependente**
- **Nega√ß√£o**: "n√£o √© bom" vs "√© bom"
- **Intensificadores**: "muito bom" vs "bom"

### 5. Visualiza√ß√£o de Dados Textuais

#### 5.1 Gr√°ficos de Frequ√™ncia
- **Histogramas**: Distribui√ß√£o de comprimentos
- **Gr√°ficos de barras**: Top palavras mais frequentes
- **Curvas de Zipf**: Lei de pot√™ncia
- **Box plots**: Estat√≠sticas descritivas

#### 5.2 Nuvens de Palavras
- **Word clouds**: Visualiza√ß√£o intuitiva
- **Configura√ß√µes**: Cores, fontes, layouts
- **M√°scaras**: Formas personalizadas
- **Filtros**: Stopwords, frequ√™ncia m√≠nima

#### 5.3 Visualiza√ß√µes Avan√ßadas
- **Heatmaps**: Matrizes de correla√ß√£o
- **Scatter plots**: Rela√ß√µes entre m√©tricas
- **Time series**: Evolu√ß√£o temporal
- **Network graphs**: Rela√ß√µes entre entidades

### 6. An√°lise de Corpus

#### 6.1 Compara√ß√£o de Corpus
- **Diferen√ßas vocabulares**
- **Palavras distintivas**
- **An√°lise temporal**
- **Segmenta√ß√£o por dom√≠nio**

#### 6.2 Detec√ß√£o de T√≥picos (B√°sica)
- **TF-IDF para t√≥picos**
- **Clustering de documentos**
- **An√°lise de co-ocorr√™ncia**
- **Prepara√ß√£o para LDA**

#### 6.3 M√©tricas de Qualidade
- **Diversidade lexical**: Type-Token Ratio (TTR)
- **Complexidade**: Comprimento m√©dio de senten√ßas
- **Legibilidade**: √çndices de Flesch, ARI
- **Coer√™ncia**: Medidas de coes√£o textual

## üõ† Ferramentas e Implementa√ß√µes

### Bibliotecas Principais
```python
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.collocations import BigramCollocationFinder
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
```

### An√°lise de N-gramas
```python
from nltk import ngrams
from nltk.util import ngrams as nltk_ngrams

def extrair_ngrams(texto, n=2):
    tokens = word_tokenize(texto.lower())
    return list(ngrams(tokens, n))

def contar_ngrams(textos, n=2, top_k=20):
    todos_ngrams = []
    for texto in textos:
        todos_ngrams.extend(extrair_ngrams(texto, n))
    
    counter = Counter(todos_ngrams)
    return counter.most_common(top_k)
```

### Medidas de Similaridade
```python
def similaridade_jaccard(set1, set2):
    intersecao = len(set1.intersection(set2))
    uniao = len(set1.union(set2))
    return intersecao / uniao if uniao > 0 else 0

def similaridade_coseno(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norma1 = np.linalg.norm(vec1)
    norma2 = np.linalg.norm(vec2)
    return dot_product / (norma1 * norma2) if norma1 > 0 and norma2 > 0 else 0
```

## üìä Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: An√°lise de Frequ√™ncia
- Calcular distribui√ß√£o de palavras
- Verificar se segue a Lei de Zipf
- Identificar palavras caracter√≠sticas por categoria

### Exerc√≠cio 2: N-gramas
- Extrair bigramas e trigramas mais frequentes
- Encontrar coloca√ß√µes significativas
- Comparar n-gramas entre diferentes textos

### Exerc√≠cio 3: Similaridade de Textos
- Implementar diferentes m√©tricas de similaridade
- Criar matriz de similaridade entre documentos
- Encontrar textos mais similares

### Exerc√≠cio 4: An√°lise de Sentimentos
- Aplicar diferentes m√©todos de an√°lise
- Comparar resultados entre abordagens
- Analisar evolu√ß√£o temporal de sentimentos

### Exerc√≠cio 5: Visualiza√ß√µes
- Criar nuvens de palavras tem√°ticas
- Plotar distribui√ß√µes estat√≠sticas
- Desenvolver dashboard interativo

## üìà M√©tricas e Avalia√ß√£o

### M√©tricas de Corpus
- **Type-Token Ratio (TTR)**: Diversidade lexical
- **Mean Length of Utterance (MLU)**: Complexidade
- **Hapax Ratio**: Propor√ß√£o de palavras √∫nicas
- **Entropy**: Medida de incerteza

### Valida√ß√£o de Resultados
- **Inspe√ß√£o manual**: Verificar top resultados
- **Compara√ß√£o com baselines**: M√©todos simples
- **Correla√ß√£o humana**: Acordo com anotadores
- **Robustez**: Estabilidade com diferentes dados

## üö® Armadilhas Comuns

### Problemas Estat√≠sticos
- **Confundir frequ√™ncia com import√¢ncia**
- **Ignorar normaliza√ß√£o por comprimento**
- **N√£o considerar distribui√ß√£o de corpus**
- **Usar m√©dias quando distribui√ß√£o √© skewed**

### Problemas de Interpreta√ß√£o
- **Assumir causalidade de correla√ß√£o**
- **Generalizar de amostras pequenas**
- **Ignorar contexto lingu√≠stico**
- **N√£o validar com especialistas**

## üìñ Leituras Complementares

### Livros
- "Foundations of Statistical Natural Language Processing" - Manning & Sch√ºtze
- "Text Analysis with R for Students of Literature" - Jockers
- "Quantitative Corpus Linguistics with R" - Gries

### Papers
- "Zipf's Word Frequency Law in Natural Language" - Piantadosi (2014)
- "The Mathematics of Statistical Machine Translation" - Brown et al. (1993)
- "Pointwise Mutual Information" - Church & Hanks (1990)

## üéØ Pr√≥ximos Passos

No **M√≥dulo 4**, vamos aprender sobre representa√ß√£o vetorial de texto, incluindo Bag of Words, TF-IDF e word embeddings.

---

**Dica**: Execute o notebook `03_analise_estatistica.ipynb` para aplicar todos os conceitos na pr√°tica! 