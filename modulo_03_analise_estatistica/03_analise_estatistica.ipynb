{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# MÃ³dulo 3: AnÃ¡lise EstatÃ­stica de Texto - Notebook PrÃ¡tico\n",
        "\n",
        "## ðŸŽ¯ Objetivos\n",
        "- Implementar anÃ¡lise de frequÃªncia e Lei de Zipf\n",
        "- Extrair e analisar n-gramas e colocaÃ§Ãµes\n",
        "- Calcular medidas de similaridade entre textos\n",
        "- Aplicar anÃ¡lise de sentimentos bÃ¡sica\n",
        "- Criar visualizaÃ§Ãµes eficazes de dados textuais\n",
        "\n",
        "## ðŸ“š ConteÃºdo\n",
        "1. AnÃ¡lise de frequÃªncia e Lei de Zipf\n",
        "2. N-gramas e colocaÃ§Ãµes\n",
        "3. Medidas de similaridade\n",
        "4. AnÃ¡lise de sentimentos\n",
        "5. VisualizaÃ§Ãµes avanÃ§adas\n",
        "6. AnÃ¡lise comparativa de corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImportaÃ§Ãµes para anÃ¡lise estatÃ­stica\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize, ngrams\n",
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "# VisualizaÃ§Ãµes\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ML for similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Importar datasets\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from datasets.textos_exemplo import *\n",
        "from utils.nlp_utils import *\n",
        "\n",
        "print(\"ðŸ“Š Bibliotecas para anÃ¡lise estatÃ­stica carregadas!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ“Š 1. AnÃ¡lise de FrequÃªncia e Lei de Zipf\n",
        "\n",
        "Vamos implementar anÃ¡lise de frequÃªncia e verificar se nossos dados seguem a Lei de Zipf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar corpus para anÃ¡lise\n",
        "corpus = textos_noticias + textos_reviews + textos_academicos\n",
        "\n",
        "# Tokenizar todo o corpus\n",
        "all_tokens = []\n",
        "for texto in corpus:\n",
        "    tokens = word_tokenize(texto.lower())\n",
        "    # Filtrar apenas palavras (remover pontuaÃ§Ã£o)\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"ðŸ“Š Corpus Statistics:\")\n",
        "print(f\"Total de documentos: {len(corpus)}\")\n",
        "print(f\"Total de tokens: {len(all_tokens):,}\")\n",
        "print(f\"VocabulÃ¡rio Ãºnico: {len(set(all_tokens)):,}\")\n",
        "print(f\"Type-Token Ratio: {len(set(all_tokens))/len(all_tokens):.4f}\")\n",
        "\n",
        "# Calcular frequÃªncias\n",
        "freq_dist = Counter(all_tokens)\n",
        "\n",
        "# Top 20 palavras mais frequentes\n",
        "print(\"\\nðŸ” Top 20 palavras mais frequentes:\")\n",
        "for word, freq in freq_dist.most_common(20):\n",
        "    print(f\"{word:15}: {freq:6,} ({freq/len(all_tokens)*100:.2f}%)\")\n",
        "\n",
        "# Implementar anÃ¡lise da Lei de Zipf\n",
        "def analyze_zipf_law(freq_dist, top_n=100):\n",
        "    \"\"\"Analisa se a distribuiÃ§Ã£o segue a Lei de Zipf\"\"\"\n",
        "    \n",
        "    # Extrair frequÃªncias e rankings\n",
        "    frequencies = [freq for word, freq in freq_dist.most_common(top_n)]\n",
        "    ranks = list(range(1, len(frequencies) + 1))\n",
        "    \n",
        "    # Lei de Zipf: freq = C / rank\n",
        "    # Em log: log(freq) = log(C) - log(rank)\n",
        "    log_freq = np.log(frequencies)\n",
        "    log_rank = np.log(ranks)\n",
        "    \n",
        "    # RegressÃ£o linear para estimar o expoente\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(log_rank, log_freq)\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ AnÃ¡lise da Lei de Zipf (top {top_n} palavras):\")\n",
        "    print(f\"Expoente (slope): {slope:.3f}\")\n",
        "    print(f\"RÂ²: {r_value**2:.3f}\")\n",
        "    print(f\"Lei de Zipf ideal: slope = -1.0\")\n",
        "    print(f\"Desvio da lei ideal: {abs(slope + 1):.3f}\")\n",
        "    \n",
        "    return frequencies, ranks, slope, r_value**2\n",
        "\n",
        "frequencies, ranks, slope, r2 = analyze_zipf_law(freq_dist)\n",
        "\n",
        "# Visualizar Lei de Zipf\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: DistribuiÃ§Ã£o de frequÃªncias (escala normal)\n",
        "ax1.plot(ranks[:50], frequencies[:50], 'bo-', alpha=0.7)\n",
        "ax1.set_xlabel('Ranking da Palavra')\n",
        "ax1.set_ylabel('FrequÃªncia')\n",
        "ax1.set_title('DistribuiÃ§Ã£o de FrequÃªncias (Top 50)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Lei de Zipf (escala log-log)\n",
        "ax2.loglog(ranks, frequencies, 'ro-', alpha=0.7, label='Dados observados')\n",
        "\n",
        "# Linha teÃ³rica da Lei de Zipf\n",
        "zipf_theoretical = frequencies[0] / np.array(ranks)\n",
        "ax2.loglog(ranks, zipf_theoretical, 'b--', alpha=0.7, label='Lei de Zipf ideal')\n",
        "\n",
        "ax2.set_xlabel('Ranking (log)')\n",
        "ax2.set_ylabel('FrequÃªncia (log)')\n",
        "ax2.set_title(f'Lei de Zipf (RÂ² = {r2:.3f})')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# AnÃ¡lise de hapax legomena (palavras que aparecem apenas uma vez)\n",
        "hapax_count = sum(1 for freq in freq_dist.values() if freq == 1)\n",
        "hapax_ratio = hapax_count / len(freq_dist)\n",
        "\n",
        "print(f\"\\nðŸ”¤ AnÃ¡lise de Hapax Legomena:\")\n",
        "print(f\"Palavras com frequÃªncia 1: {hapax_count:,}\")\n",
        "print(f\"ProporÃ§Ã£o de hapax: {hapax_ratio:.3f}\")\n",
        "print(f\"Exemplos de hapax: {list(freq_dist.keys())[:10] if hapax_count > 0 else 'Nenhum'}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
