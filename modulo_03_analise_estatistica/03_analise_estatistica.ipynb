{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Módulo 3: Análise Estatística de Texto - Notebook Prático\n",
        "\n",
        "## 🎯 Objetivos\n",
        "- Implementar análise de frequência e Lei de Zipf\n",
        "- Extrair e analisar n-gramas e colocações\n",
        "- Calcular medidas de similaridade entre textos\n",
        "- Aplicar análise de sentimentos básica\n",
        "- Criar visualizações eficazes de dados textuais\n",
        "\n",
        "## 📚 Conteúdo\n",
        "1. Análise de frequência e Lei de Zipf\n",
        "2. N-gramas e colocações\n",
        "3. Medidas de similaridade\n",
        "4. Análise de sentimentos\n",
        "5. Visualizações avançadas\n",
        "6. Análise comparativa de corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importações para análise estatística\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize, ngrams\n",
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Visualizações\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ML for similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Importar datasets\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from datasets.textos_exemplo import *\n",
        "from utils.nlp_utils import *\n",
        "\n",
        "print(\"📊 Bibliotecas para análise estatística carregadas!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 1. Análise de Frequência e Lei de Zipf\n",
        "\n",
        "Vamos implementar análise de frequência e verificar se nossos dados seguem a Lei de Zipf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar corpus para análise\n",
        "corpus = textos_noticias + textos_reviews + textos_academicos\n",
        "\n",
        "# Tokenizar todo o corpus\n",
        "all_tokens = []\n",
        "for texto in corpus:\n",
        "    tokens = word_tokenize(texto.lower())\n",
        "    # Filtrar apenas palavras (remover pontuação)\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"📊 Corpus Statistics:\")\n",
        "print(f\"Total de documentos: {len(corpus)}\")\n",
        "print(f\"Total de tokens: {len(all_tokens):,}\")\n",
        "print(f\"Vocabulário único: {len(set(all_tokens)):,}\")\n",
        "print(f\"Type-Token Ratio: {len(set(all_tokens))/len(all_tokens):.4f}\")\n",
        "\n",
        "# Calcular frequências\n",
        "freq_dist = Counter(all_tokens)\n",
        "\n",
        "# Top 20 palavras mais frequentes\n",
        "print(\"\\n🔝 Top 20 palavras mais frequentes:\")\n",
        "for word, freq in freq_dist.most_common(20):\n",
        "    print(f\"{word:15}: {freq:6,} ({freq/len(all_tokens)*100:.2f}%)\")\n",
        "\n",
        "# Implementar análise da Lei de Zipf\n",
        "def analyze_zipf_law(freq_dist, top_n=100):\n",
        "    \"\"\"Analisa se a distribuição segue a Lei de Zipf\"\"\"\n",
        "    \n",
        "    # Extrair frequências e rankings\n",
        "    frequencies = [freq for word, freq in freq_dist.most_common(top_n)]\n",
        "    ranks = list(range(1, len(frequencies) + 1))\n",
        "    \n",
        "    # Lei de Zipf: freq = C / rank\n",
        "    # Em log: log(freq) = log(C) - log(rank)\n",
        "    log_freq = np.log(frequencies)\n",
        "    log_rank = np.log(ranks)\n",
        "    \n",
        "    # Regressão linear para estimar o expoente\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(log_rank, log_freq)\n",
        "    \n",
        "    print(f\"\\n📈 Análise da Lei de Zipf (top {top_n} palavras):\")\n",
        "    print(f\"Expoente (slope): {slope:.3f}\")\n",
        "    print(f\"R²: {r_value**2:.3f}\")\n",
        "    print(f\"Lei de Zipf ideal: slope = -1.0\")\n",
        "    print(f\"Desvio da lei ideal: {abs(slope + 1):.3f}\")\n",
        "    \n",
        "    return frequencies, ranks, slope, r_value**2\n",
        "\n",
        "frequencies, ranks, slope, r2 = analyze_zipf_law(freq_dist)\n",
        "\n",
        "# Visualizar Lei de Zipf\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Distribuição de frequências (escala normal)\n",
        "ax1.plot(ranks[:50], frequencies[:50], 'bo-', alpha=0.7)\n",
        "ax1.set_xlabel('Ranking da Palavra')\n",
        "ax1.set_ylabel('Frequência')\n",
        "ax1.set_title('Distribuição de Frequências (Top 50)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Lei de Zipf (escala log-log)\n",
        "ax2.loglog(ranks, frequencies, 'ro-', alpha=0.7, label='Dados observados')\n",
        "\n",
        "# Linha teórica da Lei de Zipf\n",
        "zipf_theoretical = frequencies[0] / np.array(ranks)\n",
        "ax2.loglog(ranks, zipf_theoretical, 'b--', alpha=0.7, label='Lei de Zipf ideal')\n",
        "\n",
        "ax2.set_xlabel('Ranking (log)')\n",
        "ax2.set_ylabel('Frequência (log)')\n",
        "ax2.set_title(f'Lei de Zipf (R² = {r2:.3f})')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análise de hapax legomena (palavras que aparecem apenas uma vez)\n",
        "hapax_count = sum(1 for freq in freq_dist.values() if freq == 1)\n",
        "hapax_ratio = hapax_count / len(freq_dist)\n",
        "\n",
        "print(f\"\\n🔤 Análise de Hapax Legomena:\")\n",
        "print(f\"Palavras com frequência 1: {hapax_count:,}\")\n",
        "print(f\"Proporção de hapax: {hapax_ratio:.3f}\")\n",
        "print(f\"Exemplos de hapax: {list(freq_dist.keys())[:10] if hapax_count > 0 else 'Nenhum'}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
