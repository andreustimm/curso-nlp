{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# M√≥dulo 2: Pr√©-processamento de Texto\n",
        "\n",
        "## üéØ Objetivos\n",
        "- Dominar t√©cnicas de limpeza de texto\n",
        "- Implementar tokeniza√ß√£o eficaz\n",
        "- Aplicar stemming e lemmatiza√ß√£o\n",
        "- Normalizar e padronizar textos\n",
        "- Lidar com diferentes idiomas e caracteres especiais\n",
        "\n",
        "## üìö Conte√∫do Pr√°tico\n",
        "1. Limpeza b√°sica de texto\n",
        "2. Tokeniza√ß√£o avan√ßada\n",
        "3. Remo√ß√£o de stopwords\n",
        "4. Stemming vs Lemmatiza√ß√£o\n",
        "5. Normaliza√ß√£o e padroniza√ß√£o\n",
        "6. Pipeline completo de pr√©-processamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes necess√°rias\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Baixar recursos necess√°rios do NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('rslp', quiet=True)\n",
        "\n",
        "# Carregar modelo do spaCy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Importar datasets de exemplo\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from datasets.textos_exemplo import *\n",
        "from utils.nlp_utils import *\n",
        "\n",
        "print(\"‚úÖ Todas as bibliotecas carregadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üßπ 1. Limpeza B√°sica de Texto\n",
        "\n",
        "Vamos come√ßar com t√©cnicas fundamentais de limpeza de texto usando nossos dados de exemplo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos trabalhar com textos ruidosos para demonstrar a limpeza\n",
        "textos_sujos = textos_ruidosos\n",
        "\n",
        "print(\"üìù Textos originais (com ru√≠do):\")\n",
        "for i, texto in enumerate(textos_sujos[:3]):\n",
        "    print(f\"\\n{i+1}. {texto}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Fun√ß√£o de limpeza b√°sica\n",
        "def limpeza_basica(texto):\n",
        "    \"\"\"\n",
        "    Aplica limpeza b√°sica ao texto:\n",
        "    - Remove URLs\n",
        "    - Remove emails  \n",
        "    - Remove caracteres especiais\n",
        "    - Normaliza espa√ßos\n",
        "    \"\"\"\n",
        "    # Remover URLs\n",
        "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remover emails\n",
        "    texto = re.sub(r'\\S+@\\S+', '', texto)\n",
        "    \n",
        "    # Remover caracteres especiais (manter apenas letras, n√∫meros e espa√ßos)\n",
        "    texto = re.sub(r'[^a-zA-Z√Ä-√ø0-9\\s]', '', texto)\n",
        "    \n",
        "    # Normalizar espa√ßos m√∫ltiplos\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    \n",
        "    # Remover espa√ßos no in√≠cio e fim\n",
        "    texto = texto.strip()\n",
        "    \n",
        "    return texto\n",
        "\n",
        "# Aplicar limpeza\n",
        "textos_limpos = [limpeza_basica(texto) for texto in textos_sujos]\n",
        "\n",
        "print(\"‚ú® Textos ap√≥s limpeza b√°sica:\")\n",
        "for i, texto in enumerate(textos_limpos[:3]):\n",
        "    print(f\"\\n{i+1}. {texto}\")\n",
        "\n",
        "# Comparar antes e depois\n",
        "print(f\"\\nüìä Estat√≠sticas:\")\n",
        "print(f\"Texto original m√©dio: {np.mean([len(t) for t in textos_sujos]):.1f} caracteres\")\n",
        "print(f\"Texto limpo m√©dio: {np.mean([len(t) for t in textos_limpos]):.1f} caracteres\")\n",
        "print(f\"Redu√ß√£o m√©dia: {(1 - np.mean([len(t) for t in textos_limpos])/np.mean([len(t) for t in textos_sujos]))*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# M√≥dulo 2: Pr√©-processamento de Texto - Notebook Pr√°tico\n",
        "\n",
        "## üéØ Objetivos\n",
        "- Implementar t√©cnicas avan√ßadas de limpeza de texto\n",
        "- Comparar diferentes m√©todos de tokeniza√ß√£o\n",
        "- Aplicar stemming e lemmatiza√ß√£o\n",
        "- Trabalhar com express√µes regulares\n",
        "- Criar pipelines de pr√©-processamento robustos\n",
        "\n",
        "## üìö Conte√∫do\n",
        "1. Configura√ß√£o e dados de exemplo\n",
        "2. Limpeza e normaliza√ß√£o de texto\n",
        "3. Tokeniza√ß√£o avan√ßada\n",
        "4. Stemming vs Lemmatiza√ß√£o\n",
        "5. Remo√ß√£o inteligente de stopwords\n",
        "6. Pipeline completo de pr√©-processamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes necess√°rias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download recursos do NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('rslp', quiet=True)\n",
        "\n",
        "# Importa√ß√µes espec√≠ficas do NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, RSLPStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Configurar visualiza√ß√£o\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "\n",
        "# Carregar modelo do spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "    print(\"‚úÖ Modelo em portugu√™s carregado!\")\n",
        "except OSError:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"‚ö†Ô∏è Usando modelo em ingl√™s\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
