# M√≥dulo 2: Pr√©-processamento de Texto

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Compreender a import√¢ncia do pr√©-processamento em NLP
- Implementar t√©cnicas de limpeza e normaliza√ß√£o de texto
- Aplicar tokeniza√ß√£o avan√ßada
- Utilizar stemming e lemmatiza√ß√£o eficientemente
- Trabalhar com express√µes regulares para processamento de texto
- Remover stopwords de forma inteligente

## üìö Conte√∫do Te√≥rico

### 1. Import√¢ncia do Pr√©-processamento

O pr√©-processamento √© uma etapa crucial em qualquer pipeline de NLP, pois:

- **Reduz o ru√≠do**: Remove caracteres irrelevantes e padroniza o texto
- **Normaliza dados**: Garante consist√™ncia no formato dos dados
- **Melhora performance**: Reduz dimensionalidade e complexidade
- **Facilita an√°lise**: Prepara texto para algoritmos de ML/DL

### 2. Etapas do Pr√©-processamento

#### 2.1 Limpeza de Texto

**Remo√ß√£o de elementos indesejados:**
- HTML tags: `<div>`, `<p>`, `<br>`
- URLs: `http://exemplo.com`
- Men√ß√µes: `@usuario`
- Hashtags: `#nlp`
- Emojis e caracteres especiais
- N√∫meros (quando irrelevantes)

**Normaliza√ß√£o de caracteres:**
- Convers√£o para min√∫sculas/mai√∫sculas
- Remo√ß√£o de acentos
- Normaliza√ß√£o de espa√ßos em branco
- Corre√ß√£o de encoding

#### 2.2 Tokeniza√ß√£o

**Defini√ß√£o**: Processo de dividir texto em unidades menores (tokens)

**Tipos de tokeniza√ß√£o:**
- **Por palavras**: Mais comum
- **Por senten√ßas**: Para an√°lise sint√°tica
- **Por subpalavras**: BPE, WordPiece
- **Por caracteres**: Para idiomas sem espa√ßos

**Desafios:**
- Contra√ß√µes: "n√£o √©" vs "n√£o" + "√©"
- Pontua√ß√£o: "Dr. Silva" vs "Dr." + "Silva"
- URLs e emails
- N√∫meros com formata√ß√£o

#### 2.3 Normaliza√ß√£o de Texto

**Case folding:**
- Convers√£o para min√∫sculas
- Preserva√ß√£o de acr√¥nimos quando necess√°rio

**Expans√£o de contra√ß√µes:**
- "don't" ‚Üí "do not"
- "I'm" ‚Üí "I am"
- "won't" ‚Üí "will not"

**Padroniza√ß√£o de abrevia√ß√µes:**
- "Dr." ‚Üí "Doctor"
- "etc." ‚Üí "et cetera"

#### 2.4 Remo√ß√£o de Stopwords

**Defini√ß√£o**: Palavras muito frequentes que geralmente n√£o carregam significado espec√≠fico

**Stopwords comuns em portugu√™s:**
- Artigos: a, o, as, os
- Preposi√ß√µes: de, para, com, em
- Pronomes: eu, voc√™, ele, ela
- Verbos auxiliares: ser, estar, ter

**Considera√ß√µes:**
- Dependem do dom√≠nio e tarefa
- Podem ser importantes em algumas an√°lises
- Listas personalizadas podem ser necess√°rias

#### 2.5 Stemming vs Lemmatiza√ß√£o

**Stemming:**
- Remove sufixos usando regras
- Mais r√°pido, menos preciso
- Pode gerar palavras inexistentes
- Exemplo: "correndo" ‚Üí "corr"

**Lemmatiza√ß√£o:**
- Reduz palavras √† forma can√¥nica
- Mais lento, mais preciso
- Considera contexto gramatical
- Exemplo: "correndo" ‚Üí "correr"

**Quando usar cada um:**
- **Stemming**: Tarefas de recupera√ß√£o de informa√ß√£o
- **Lemmatiza√ß√£o**: An√°lise sint√°tica e sem√¢ntica

### 3. Ferramentas e Bibliotecas

#### 3.1 NLTK
```python
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
```

#### 3.2 spaCy
```python
import spacy
nlp = spacy.load("pt_core_news_sm")
doc = nlp("texto")
tokens = [token.lemma_ for token in doc]
```

#### 3.3 Express√µes Regulares
```python
import re
# Remover URLs
texto = re.sub(r'http\S+', '', texto)
# Remover men√ß√µes
texto = re.sub(r'@\w+', '', texto)
```

### 4. T√©cnicas Avan√ßadas

#### 4.1 Normaliza√ß√£o Unicode
- **NFD**: Decomposi√ß√£o can√¥nica
- **NFC**: Composi√ß√£o can√¥nica
- **NFKD/NFKC**: Normaliza√ß√£o de compatibilidade

#### 4.2 Detec√ß√£o de Idioma
- Identificar automaticamente o idioma do texto
- Aplicar pr√©-processamento espec√≠fico por idioma

#### 4.3 Corre√ß√£o Ortogr√°fica
- Detec√ß√£o de erros de digita√ß√£o
- Sugest√£o de corre√ß√µes
- Normaliza√ß√£o de varia√ß√µes

#### 4.4 Segmenta√ß√£o de Senten√ßas
- Divis√£o correta em senten√ßas
- Tratamento de abrevia√ß√µes
- Pontua√ß√£o especial

### 5. Boas Pr√°ticas

#### 5.1 Pipeline Consistente
- Aplicar sempre as mesmas transforma√ß√µes
- Documentar todas as etapas
- Manter versionamento do pipeline

#### 5.2 Preserva√ß√£o de Informa√ß√£o
- Manter texto original quando poss√≠vel
- Registrar transforma√ß√µes aplicadas
- Permitir reversibilidade quando necess√°rio

#### 5.3 Valida√ß√£o
- Verificar resultados em amostra
- Testar com casos extremos
- Monitorar impacto na performance

#### 5.4 Efici√™ncia
- Paralelizar quando poss√≠vel
- Usar bibliotecas otimizadas
- Cache de resultados frequentes

## üõ† Ferramentas Espec√≠ficas

### Express√µes Regulares √öteis

```python
# URLs
r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

# E-mails
r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

# Telefones (Brasil)
r'\(?\d{2}\)?\s?\d{4,5}-?\d{4}'

# Datas
r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}'

# Hashtags
r'#\w+'

# Men√ß√µes
r'@\w+'
```

### Stopwords Customizadas

```python
# Stopwords b√°sicas + dom√≠nio espec√≠fico
stopwords_custom = stopwords.words('portuguese') + [
    'site', 'p√°gina', 'clique', 'aqui', 'mais',
    'muito', 'bem', 'bom', 'boa', 'melhor'
]
```

## üìä M√©tricas de Qualidade

### Antes vs Depois do Pr√©-processamento
- **Vocabul√°rio**: Redu√ß√£o do n√∫mero de tokens √∫nicos
- **Esparsidade**: Diminui√ß√£o da matriz de caracter√≠sticas
- **Consist√™ncia**: Padroniza√ß√£o de varia√ß√µes
- **Relev√¢ncia**: Manuten√ß√£o de informa√ß√£o importante

### Valida√ß√£o Manual
- Amostragem de resultados
- Verifica√ß√£o de casos extremos
- Revis√£o por especialistas
- Testes A/B com diferentes pipelines

## üí° Exerc√≠cios Pr√°ticos

1. **Limpeza de Tweets**: Processar dados do Twitter
2. **Normaliza√ß√£o de Reviews**: Padronizar avalia√ß√µes de produtos
3. **Pipeline Personalizado**: Criar pipeline para dom√≠nio espec√≠fico
4. **Compara√ß√£o de M√©todos**: Stemming vs Lemmatiza√ß√£o
5. **Regex Avan√ßado**: Extrair informa√ß√µes espec√≠ficas

## üö® Armadilhas Comuns

### Over-processing
- Remover informa√ß√£o relevante
- Aplicar transforma√ß√µes desnecess√°rias
- Perder contexto importante

### Under-processing
- Manter ru√≠do desnecess√°rio
- N√£o normalizar varia√ß√µes
- Ignorar caracter√≠sticas espec√≠ficas do dom√≠nio

### Inconsist√™ncia
- Aplicar regras diferentes em treino/teste
- Mudan√ßas no pipeline durante o projeto
- N√£o documentar transforma√ß√µes

## üìñ Leituras Complementares

### Artigos
- "Text Preprocessing in Python: Steps, Tools, and Examples"
- "The Art of Cleaning Your Data"
- "Regular Expressions for Text Processing"

### Documenta√ß√£o
- [NLTK Preprocessing](https://www.nltk.org/book/ch03.html)
- [spaCy Linguistic Features](https://spacy.io/usage/linguistic-features)
- [Regex Python Documentation](https://docs.python.org/3/library/re.html)

## üéØ Pr√≥ximos Passos

No **M√≥dulo 3**, vamos explorar an√°lise estat√≠stica de texto, incluindo an√°lise de frequ√™ncia, n-gramas e medidas de similaridade.

---

**Dica**: Execute o notebook `02_preprocessamento_texto.ipynb` para praticar todas as t√©cnicas! 