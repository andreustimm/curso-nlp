{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# M√≥dulo 1: Fundamentos de NLP - Notebook Pr√°tico\n",
        "\n",
        "## üéØ Objetivos\n",
        "- Configurar o ambiente de desenvolvimento\n",
        "- Explorar bibliotecas b√°sicas de NLP\n",
        "- Implementar um pipeline simples\n",
        "- Analisar caracter√≠sticas de texto\n",
        "\n",
        "## üìö Conte√∫do\n",
        "1. Configura√ß√£o e importa√ß√µes\n",
        "2. Primeiro contato com NLTK e spaCy\n",
        "3. Pipeline b√°sico de NLP\n",
        "4. An√°lise explorat√≥ria de texto\n",
        "5. Exerc√≠cios pr√°ticos\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Configura√ß√£o e Importa√ß√µes\n",
        "\n",
        "Primeiro, vamos importar todas as bibliotecas necess√°rias e verificar se est√£o funcionando corretamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes b√°sicas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√£o de visualiza√ß√£o\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"‚úÖ Bibliotecas b√°sicas importadas com sucesso!\")\n",
        "\n",
        "# Verificar vers√µes das principais bibliotecas\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bibliotecas de NLP\n",
        "try:\n",
        "    import nltk\n",
        "    print(f\"‚úÖ NLTK version: {nltk.__version__}\")\n",
        "    \n",
        "    # Download de recursos necess√°rios do NLTK\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    print(\"‚úÖ Recursos do NLTK baixados!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ùå NLTK n√£o encontrado. Execute: pip install nltk\")\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    print(f\"‚úÖ spaCy version: {spacy.__version__}\")\n",
        "    \n",
        "    # Verificar se o modelo em portugu√™s est√° dispon√≠vel\n",
        "    try:\n",
        "        nlp = spacy.load(\"pt_core_news_sm\")\n",
        "        print(\"‚úÖ Modelo em portugu√™s do spaCy carregado!\")\n",
        "    except OSError:\n",
        "        print(\"‚ö†Ô∏è Modelo em portugu√™s n√£o encontrado. Execute: python -m spacy download pt_core_news_sm\")\n",
        "        # Usar modelo em ingl√™s como fallback\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"‚úÖ Usando modelo em ingl√™s como alternativa\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ùå spaCy n√£o encontrado. Execute: pip install spacy\")\n",
        "\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "    print(\"‚úÖ TextBlob importado com sucesso!\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå TextBlob n√£o encontrado. Execute: pip install textblob\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Primeiro Contato com Bibliotecas de NLP\n",
        "\n",
        "Vamos explorar as principais funcionalidades do NLTK, spaCy e TextBlob com exemplos pr√°ticos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Texto de exemplo\n",
        "texto_exemplo = \"\"\"\n",
        "Processamento de Linguagem Natural √© uma √°rea fascinante da intelig√™ncia artificial.\n",
        "Com t√©cnicas de machine learning, podemos ensinar computadores a entender textos.\n",
        "Este curso vai te ensinar desde o b√°sico at√© t√©cnicas avan√ßadas como transformers.\n",
        "O futuro do NLP √© muito promissor!\n",
        "\"\"\"\n",
        "\n",
        "print(\"Texto de exemplo:\")\n",
        "print(texto_exemplo)\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 NLTK - Natural Language Toolkit\n",
        "print(\"=== Explorando NLTK ===\")\n",
        "\n",
        "# Tokeniza√ß√£o com NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Tokenizar senten√ßas\n",
        "sentences = sent_tokenize(texto_exemplo)\n",
        "print(\"Senten√ßas:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    if sent.strip():  # Ignorar strings vazias\n",
        "        print(f\"{i}. {sent.strip()}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "\n",
        "# Tokenizar palavras\n",
        "words = word_tokenize(texto_exemplo)\n",
        "print(f\"N√∫mero total de tokens: {len(words)}\")\n",
        "print(\"Primeiros 15 tokens:\")\n",
        "print(words[:15])\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "\n",
        "# An√°lise de frequ√™ncia\n",
        "from collections import Counter\n",
        "word_freq = Counter(words)\n",
        "print(\"Palavras mais frequentes:\")\n",
        "for word, freq in word_freq.most_common(10):\n",
        "    print(f\"'{word}': {freq}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2 spaCy - Industrial-strength NLP\n",
        "print(\"\\n=== Explorando spaCy ===\")\n",
        "\n",
        "# Processar texto com spaCy\n",
        "doc = nlp(texto_exemplo)\n",
        "\n",
        "print(\"An√°lise lingu√≠stica com spaCy:\")\n",
        "print(\"Token | Lemma | POS | Tag | Dep | Shape | Alpha | Stop\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for token in doc[:15]:  # Primeiros 15 tokens\n",
        "    print(f\"{token.text:<8} | {token.lemma_:<8} | {token.pos_:<4} | {token.tag_:<4} | {token.dep_:<6} | {token.shape_:<6} | {token.is_alpha} | {token.is_stop}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "\n",
        "# Entidades nomeadas\n",
        "print(\"Entidades Nomeadas encontradas:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"'{ent.text}' - {ent.label_} ({ent.start_char}-{ent.end_char})\")\n",
        "\n",
        "if not doc.ents:\n",
        "    print(\"Nenhuma entidade nomeada encontrada no texto exemplo.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.3 TextBlob - Simplified text processing\n",
        "print(\"\\n=== Explorando TextBlob ===\")\n",
        "\n",
        "blob = TextBlob(texto_exemplo)\n",
        "\n",
        "print(\"An√°lise com TextBlob:\")\n",
        "print(f\"N√∫mero de senten√ßas: {len(blob.sentences)}\")\n",
        "print(f\"N√∫mero de palavras: {len(blob.words)}\")\n",
        "\n",
        "print(\"\\nSenten√ßas detectadas:\")\n",
        "for i, sentence in enumerate(blob.sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30)\n",
        "\n",
        "# An√°lise de sentimentos\n",
        "print(\"An√°lise de Sentimentos:\")\n",
        "sentiment = blob.sentiment\n",
        "print(f\"Polaridade: {sentiment.polarity:.3f} ([-1.0 = negativo, 1.0 = positivo])\")\n",
        "print(f\"Subjetividade: {sentiment.subjectivity:.3f} ([0.0 = objetivo, 1.0 = subjetivo])\")\n",
        "\n",
        "# Interpreta√ß√£o\n",
        "if sentiment.polarity > 0.1:\n",
        "    sentimento = \"Positivo\"\n",
        "elif sentiment.polarity < -0.1:\n",
        "    sentimento = \"Negativo\"\n",
        "else:\n",
        "    sentimento = \"Neutro\"\n",
        "    \n",
        "print(f\"Sentimento geral: {sentimento}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimplePipelineNLP:\n",
        "    \"\"\"\n",
        "    Pipeline b√°sico de NLP que demonstra as etapas fundamentais\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        from nltk.corpus import stopwords\n",
        "        from nltk.stem import PorterStemmer\n",
        "        \n",
        "        self.stopwords = set(stopwords.words('portuguese'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        \n",
        "    def limpar_texto(self, texto):\n",
        "        \"\"\"Etapa 1: Limpeza b√°sica do texto\"\"\"\n",
        "        # Converter para min√∫sculas\n",
        "        texto = texto.lower()\n",
        "        \n",
        "        # Remover caracteres especiais (manter apenas letras e espa√ßos)\n",
        "        texto = re.sub(r'[^a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß\\s]', '', texto)\n",
        "        \n",
        "        # Remover espa√ßos extras\n",
        "        texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "        \n",
        "        return texto\n",
        "    \n",
        "    def tokenizar(self, texto):\n",
        "        \"\"\"Etapa 2: Tokeniza√ß√£o\"\"\"\n",
        "        tokens = word_tokenize(texto)\n",
        "        return tokens\n",
        "    \n",
        "    def remover_stopwords(self, tokens):\n",
        "        \"\"\"Etapa 3: Remo√ß√£o de stopwords\"\"\"\n",
        "        return [token for token in tokens if token not in self.stopwords]\n",
        "    \n",
        "    def stemming(self, tokens):\n",
        "        \"\"\"Etapa 4: Stemming\"\"\"\n",
        "        return [self.stemmer.stem(token) for token in tokens]\n",
        "    \n",
        "    def processar(self, texto):\n",
        "        \"\"\"Pipeline completo\"\"\"\n",
        "        print(\"Pipeline de NLP:\")\n",
        "        print(\"=\" * 40)\n",
        "        \n",
        "        print(f\"Texto original:\\n{texto}\\n\")\n",
        "        \n",
        "        # Etapa 1: Limpeza\n",
        "        texto_limpo = self.limpar_texto(texto)\n",
        "        print(f\"1. Texto limpo:\\n{texto_limpo}\\n\")\n",
        "        \n",
        "        # Etapa 2: Tokeniza√ß√£o\n",
        "        tokens = self.tokenizar(texto_limpo)\n",
        "        print(f\"2. Tokens ({len(tokens)}):\\n{tokens}\\n\")\n",
        "        \n",
        "        # Etapa 3: Remo√ß√£o de stopwords\n",
        "        tokens_sem_stopwords = self.remover_stopwords(tokens)\n",
        "        print(f\"3. Sem stopwords ({len(tokens_sem_stopwords)}):\\n{tokens_sem_stopwords}\\n\")\n",
        "        \n",
        "        # Etapa 4: Stemming\n",
        "        tokens_stemmed = self.stemming(tokens_sem_stopwords)\n",
        "        print(f\"4. Com stemming ({len(tokens_stemmed)}):\\n{tokens_stemmed}\\n\")\n",
        "        \n",
        "        return {\n",
        "            'original': texto,\n",
        "            'limpo': texto_limpo,\n",
        "            'tokens': tokens,\n",
        "            'sem_stopwords': tokens_sem_stopwords,\n",
        "            'stemmed': tokens_stemmed\n",
        "        }\n",
        "\n",
        "# Testando o pipeline\n",
        "pipeline = SimplePipelineNLP()\n",
        "resultado = pipeline.processar(texto_exemplo)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. An√°lise Explorat√≥ria de Texto\n",
        "\n",
        "Vamos criar algumas visualiza√ß√µes para entender melhor as caracter√≠sticas do texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um dataset maior para an√°lise\n",
        "textos_exemplo = [\n",
        "    \"Intelig√™ncia artificial est√° transformando o mundo dos neg√≥cios.\",\n",
        "    \"Machine learning permite que computadores aprendam sem programa√ß√£o expl√≠cita.\",\n",
        "    \"Deep learning usa redes neurais profundas para resolver problemas complexos.\",\n",
        "    \"Natural Language Processing ajuda computadores a entender linguagem humana.\",\n",
        "    \"O futuro da tecnologia √© muito promissor e cheio de oportunidades.\",\n",
        "    \"Python √© uma linguagem de programa√ß√£o muito popular para data science.\",\n",
        "    \"Algoritmos de IA podem automatizar tarefas repetitivas e complexas.\",\n",
        "    \"Big data requer ferramentas especializadas para an√°lise eficiente.\",\n",
        "    \"Cloud computing oferece escalabilidade para aplica√ß√µes modernas.\",\n",
        "    \"Chatbots est√£o revolucionando o atendimento ao cliente online.\"\n",
        "]\n",
        "\n",
        "print(f\"Dataset com {len(textos_exemplo)} textos para an√°lise\")\n",
        "print(\"Primeiros 3 textos:\")\n",
        "for i, texto in enumerate(textos_exemplo[:3], 1):\n",
        "    print(f\"{i}. {texto}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise estat√≠stica b√°sica\n",
        "import pandas as pd\n",
        "\n",
        "# Processar todos os textos\n",
        "todos_resultados = []\n",
        "for i, texto in enumerate(textos_exemplo):\n",
        "    resultado = pipeline.processar(texto)\n",
        "    todos_resultados.append({\n",
        "        'id': i,\n",
        "        'texto_original': texto,\n",
        "        'num_caracteres': len(texto),\n",
        "        'num_palavras': len(resultado['tokens']),\n",
        "        'num_palavras_sem_stopwords': len(resultado['sem_stopwords']),\n",
        "        'num_palavras_stemmed': len(resultado['stemmed'])\n",
        "    })\n",
        "\n",
        "# Criar DataFrame para an√°lise\n",
        "df_analise = pd.DataFrame(todos_resultados)\n",
        "\n",
        "print(\"Estat√≠sticas descritivas:\")\n",
        "print(df_analise[['num_caracteres', 'num_palavras', 'num_palavras_sem_stopwords']].describe())\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Distribui√ß√£o do n√∫mero de caracteres\n",
        "axes[0,0].hist(df_analise['num_caracteres'], bins=8, alpha=0.7, color='skyblue')\n",
        "axes[0,0].set_title('Distribui√ß√£o: N√∫mero de Caracteres')\n",
        "axes[0,0].set_xlabel('Caracteres')\n",
        "axes[0,0].set_ylabel('Frequ√™ncia')\n",
        "\n",
        "# Distribui√ß√£o do n√∫mero de palavras\n",
        "axes[0,1].hist(df_analise['num_palavras'], bins=8, alpha=0.7, color='lightgreen')\n",
        "axes[0,1].set_title('Distribui√ß√£o: N√∫mero de Palavras')\n",
        "axes[0,1].set_xlabel('Palavras')\n",
        "axes[0,1].set_ylabel('Frequ√™ncia')\n",
        "\n",
        "# Compara√ß√£o: antes e depois de remover stopwords\n",
        "x = range(len(df_analise))\n",
        "axes[1,0].bar(x, df_analise['num_palavras'], alpha=0.7, label='Com stopwords', color='orange')\n",
        "axes[1,0].bar(x, df_analise['num_palavras_sem_stopwords'], alpha=0.7, label='Sem stopwords', color='red')\n",
        "axes[1,0].set_title('Impacto da Remo√ß√£o de Stopwords')\n",
        "axes[1,0].set_xlabel('Texto ID')\n",
        "axes[1,0].set_ylabel('N√∫mero de Palavras')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Box plot das estat√≠sticas\n",
        "data_for_box = [df_analise['num_caracteres'], df_analise['num_palavras'], df_analise['num_palavras_sem_stopwords']]\n",
        "axes[1,1].boxplot(data_for_box, labels=['Caracteres', 'Palavras', 'Sem Stopwords'])\n",
        "axes[1,1].set_title('Box Plot das M√©tricas')\n",
        "axes[1,1].set_ylabel('Contagem')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Pipeline B√°sico de NLP\n",
        "\n",
        "Vamos implementar um pipeline simples que demonstra as etapas fundamentais do processamento de texto.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
