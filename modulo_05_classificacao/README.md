[‚¨ÖÔ∏è Voltar para o √≠ndice do curso](../README.md)

# M√≥dulo 5: Classifica√ß√£o de Texto

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Implementar classificadores Naive Bayes para texto
- Aplicar SVM e regress√£o log√≠stica em NLP
- Usar ensemble methods para melhorar performance
- Realizar valida√ß√£o cruzada e avaliar modelos
- Lidar com dados desbalanceados
- Otimizar hiperpar√¢metros de classificadores

## üìö Conte√∫do Te√≥rico

### 1. Fundamentos da Classifica√ß√£o de Texto

#### 1.1 Defini√ß√£o da Tarefa
Classifica√ß√£o de texto √© a tarefa de atribuir categorias predefinidas a documentos baseado em seu conte√∫do.

**Tipos de Classifica√ß√£o:**
- **Bin√°ria**: Duas classes (spam/n√£o-spam)
- **Multi-classe**: M√∫ltiplas classes mutuamente exclusivas
- **Multi-label**: M√∫ltiplas classes n√£o exclusivas
- **Hier√°rquica**: Classes organizadas em taxonomia

#### 1.2 Pipeline de Classifica√ß√£o
1. **Coleta e rotula√ß√£o** de dados
2. **Pr√©-processamento** de texto
3. **Extra√ß√£o de features** (representa√ß√£o)
4. **Divis√£o** treino/valida√ß√£o/teste
5. **Treinamento** do modelo
6. **Avalia√ß√£o** e otimiza√ß√£o
7. **Deploy** e monitoramento

### 2. Naive Bayes para Texto

#### 2.1 Teorema de Bayes
P(classe|documento) = P(documento|classe) √ó P(classe) / P(documento)

**Componentes:**
- **P(classe)**: Probabilidade a priori
- **P(documento|classe)**: Verossimilhan√ßa
- **P(documento)**: Evid√™ncia (constante)

#### 2.2 Assumption de Independ√™ncia
"Naive" porque assume independ√™ncia condicional entre features.

**Na pr√°tica:**
- Palavras s√£o tratadas independentemente
- Simplifica√ß√£o forte mas funciona bem
- Reduz complexidade computacional

#### 2.3 Variantes do Naive Bayes

**Multinomial NB:**
- Para contagens de palavras
- Boa para classifica√ß√£o de t√≥picos
- Assume distribui√ß√£o multinomial

**Bernoulli NB:**
- Para presen√ßa/aus√™ncia de palavras
- Bom para textos curtos
- Features bin√°rias

**Gaussian NB:**
- Para features cont√≠nuas
- Assume distribui√ß√£o normal
- √ötil com embeddings

#### 2.4 Suaviza√ß√£o (Smoothing)
Evita probabilidades zero para palavras n√£o vistas.

**Add-one (Laplace) Smoothing:**
P(palavra|classe) = (count + 1) / (total + vocab_size)

**Add-alpha Smoothing:**
P(palavra|classe) = (count + Œ±) / (total + Œ± √ó vocab_size)

### 3. Support Vector Machines (SVM)

#### 3.1 Conceito B√°sico
SVM encontra hiperplano que separa classes com margem m√°xima.

**Vantagens para texto:**
- Eficaz em alta dimensionalidade
- Funciona bem com dados esparsos
- Resistente a overfitting
- Suporte a kernels n√£o-lineares

#### 3.2 Kernels para Texto

**Linear Kernel:**
- K(x,y) = x¬∑y
- Mais comum para texto
- Interpret√°vel e eficiente

**RBF (Gaussian) Kernel:**
- K(x,y) = exp(-Œ≥||x-y||¬≤)
- Para rela√ß√µes n√£o-lineares
- Requer normaliza√ß√£o

**Polynomial Kernel:**
- K(x,y) = (x¬∑y + c)^d
- Captura intera√ß√µes entre features
- Cuidado com overfitting

#### 3.3 Hiperpar√¢metros
- **C**: Regulariza√ß√£o (trade-off bias-variance)
- **gamma**: Para kernels RBF
- **degree**: Para kernels polinomiais

### 4. Regress√£o Log√≠stica

#### 4.1 Modelo Linear Generalizado
Usa fun√ß√£o log√≠stica para mapear scores para probabilidades.

**Fun√ß√£o Sigmoide:**
P(y=1|x) = 1 / (1 + e^(-w·µÄx))

#### 4.2 Vantagens
- Retorna probabilidades calibradas
- Interpret√°vel (coeficientes = import√¢ncia)
- Baseline forte para muitas tarefas
- Treinamento eficiente

#### 4.3 Regulariza√ß√£o

**L1 (Lasso):**
- Penalty: Œª‚àë|w·µ¢|
- Feature selection autom√°tica
- Produz esparsidade

**L2 (Ridge):**
- Penalty: Œª‚àëw·µ¢¬≤
- Previne overfitting
- Mant√©m todas as features

**Elastic Net:**
- Combina L1 e L2
- Œ±‚àë|w·µ¢| + (1-Œ±)‚àëw·µ¢¬≤

### 5. Ensemble Methods

#### 5.1 Voting Classifiers

**Hard Voting:**
- Voto majorit√°rio
- Cada modelo contribui 1 voto

**Soft Voting:**
- M√©dia das probabilidades
- Usa confian√ßa dos modelos

#### 5.2 Bagging

**Random Forest:**
- M√∫ltiplas √°rvores
- Bootstrap sampling
- Feature randomness

**Extra Trees:**
- √Årvores completamente aleat√≥rias
- Splits aleat√≥rios
- Menos overfitting

#### 5.3 Boosting

**AdaBoost:**
- Foca em exemplos dif√≠ceis
- Pesos adaptativos
- Combina weak learners

**Gradient Boosting:**
- XGBoost, LightGBM, CatBoost
- Otimiza√ß√£o sequencial
- Estado da arte em tabular

### 6. Avalia√ß√£o de Modelos

#### 6.1 M√©tricas para Classifica√ß√£o

**Accuracy:**
- (TP + TN) / (TP + TN + FP + FN)
- √ötil quando classes balanceadas

**Precision:**
- TP / (TP + FP)
- "Dos que classifiquei como positivos, quantos realmente s√£o?"

**Recall (Sensitivity):**
- TP / (TP + FN)
- "Dos positivos reais, quantos consegui identificar?"

**F1-Score:**
- 2 √ó (Precision √ó Recall) / (Precision + Recall)
- M√©dia harm√¥nica de precision e recall

#### 6.2 M√©tricas Multi-classe

**Macro Average:**
- Calcula m√©trica para cada classe
- M√©dia n√£o ponderada
- Trata classes igualmente

**Micro Average:**
- Agrega TPs, FPs, FNs globalmente
- Calculado uma √∫nica m√©trica
- Favorece classes majorit√°rias

**Weighted Average:**
- M√©dia ponderada por suporte
- Considera distribui√ß√£o de classes

#### 6.3 Valida√ß√£o Cruzada

**K-Fold:**
- Divide dados em k parti√ß√µes
- k rounds de treino/valida√ß√£o
- M√©dia dos resultados

**Stratified K-Fold:**
- Mant√©m propor√ß√£o de classes
- Importante para dados desbalanceados

**Time Series Split:**
- Para dados temporais
- Treino sempre anterior √† valida√ß√£o

### 7. Dados Desbalanceados

#### 7.1 Problemas
- Accuracy enganosa
- Bias para classe majorit√°ria
- Recall baixo para classe minorit√°ria

#### 7.2 T√©cnicas de Sampling

**Under-sampling:**
- Remove exemplos da classe majorit√°ria
- SMOTE, Tomek Links, EditedNearestNeighbours

**Over-sampling:**
- Adiciona exemplos da classe minorit√°ria
- SMOTE, ADASYN, BorderlineSMOTE

**Hybrid:**
- Combina under e over-sampling
- SMOTEENN, SMOTETomek

#### 7.3 T√©cnicas Algor√≠tmicas

**Class Weights:**
- Penaliza erros em classes minorit√°rias
- class_weight='balanced'

**Threshold Adjustment:**
- Ajusta threshold de decis√£o
- Otimiza para m√©trica espec√≠fica

**Cost-Sensitive Learning:**
- Matrix de custos customizada
- Penaliza diferentes tipos de erro

## üõ† Implementa√ß√µes Pr√°ticas

### Pipeline Completo
```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('classifier', LogisticRegression())
])

parameters = {
    'tfidf__ngram_range': [(1,1), (1,2)],
    'classifier__C': [0.1, 1, 10]
}

grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='f1_macro')
```

### Naive Bayes
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)
classifier = MultinomialNB(alpha=1.0)
classifier.fit(X, y)
```

### SVM
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Para embeddings (features densas)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_dense)
svm = SVC(kernel='rbf', C=1.0, probability=True)
svm.fit(X_scaled, y)
```

## üìä Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Classifica√ß√£o de Sentimentos
- Dataset de reviews de produtos
- Comparar Naive Bayes, SVM, Logistic Regression
- An√°lise de features mais importantes

### Exerc√≠cio 2: Classifica√ß√£o de Not√≠cias
- Dataset multi-classe
- Ensemble de diferentes algoritmos
- Otimiza√ß√£o de hiperpar√¢metros

### Exerc√≠cio 3: Detec√ß√£o de Spam
- Dataset desbalanceado
- T√©cnicas para dados desbalanceados
- An√°lise de falsos positivos/negativos

### Exerc√≠cio 4: Pipeline Completo
- From raw text to predictions
- Cross-validation robusta
- Feature engineering avan√ßado

## üéØ Pr√≥ximos Passos

No **M√≥dulo 6**, exploraremos modelos de sequ√™ncia como HMMs e CRFs para tarefas estruturadas.

---

**Dica**: Execute o notebook `05_classificacao_texto.ipynb` para implementar todos os algoritmos! 

[‚¨ÖÔ∏è Voltar para o √≠ndice do curso](../README.md) 