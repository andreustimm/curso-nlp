[‚¨ÖÔ∏è Voltar para o √≠ndice do curso](../README.md)

# M√≥dulo 8: Transformers e Modelos Pr√©-treinados

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Compreender a arquitetura Transformer em detalhes
- Utilizar modelos pr√©-treinados (BERT, GPT, RoBERTa)
- Implementar fine-tuning para tarefas espec√≠ficas
- Aplicar transfer learning em NLP
- Usar a biblioteca Transformers do Hugging Face
- Otimizar modelos grandes para produ√ß√£o

## üìö Conte√∫do Te√≥rico

### 1. Arquitetura Transformer

#### 1.1 "Attention Is All You Need"
Paper revolucion√°rio (2017) que introduziu arquitetura baseada apenas em attention.

**Motiva√ß√£o:**
- RNNs s√£o sequenciais (n√£o paralelizam)
- CNNs t√™m receptive field limitado
- Attention pode capturar depend√™ncias longas

#### 1.2 Multi-Head Attention

**Self-Attention:**
```
Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V
```

**Multi-Head:**
```
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**Vantagens:**
- Diferentes tipos de rela√ß√µes
- Paraleliza√ß√£o completa
- Interpretabilidade via attention weights

#### 1.3 Positional Encoding
Transformers n√£o t√™m no√ß√£o inerente de posi√ß√£o.

**Sinusoidal Encoding:**
```
PE(pos,2i) = sin(pos/10000^(2i/d_model))
PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
```

#### 1.4 Arquitetura Completa
```
Input ‚Üí Embedding + Positional Encoding
‚Üì
Multi-Head Attention
‚Üì
Add & Norm
‚Üì
Feed Forward Network
‚Üì
Add & Norm
‚Üì
(Repetir N vezes)
‚Üì
Output
```

### 2. BERT (Bidirectional Encoder Representations from Transformers)

#### 2.1 Caracter√≠sticas Principais
- **Bidirectional**: V√™ contexto completo
- **Pre-training**: MLM + NSP
- **Encoder-only**: Baseado em encoder do Transformer
- **Fine-tuning**: Adapt√°vel para tarefas downstream

#### 2.2 Pre-training Tasks

**Masked Language Model (MLM):**
- 15% dos tokens s√£o mascarados
- 80% ‚Üí [MASK], 10% ‚Üí palavra aleat√≥ria, 10% ‚Üí original
- Modelo prediz tokens mascarados

**Next Sentence Prediction (NSP):**
- Prediz se frase B segue frase A
- 50% casos positivos, 50% negativos
- [CLS] token para classifica√ß√£o

#### 2.3 Tokens Especiais
- **[CLS]**: In√≠cio da sequ√™ncia (classifica√ß√£o)
- **[SEP]**: Separador entre senten√ßas
- **[MASK]**: Token mascarado
- **[PAD]**: Padding
- **[UNK]**: Palavra desconhecida

#### 2.4 Varia√ß√µes do BERT
- **RoBERTa**: Remove NSP, dynamic masking
- **ALBERT**: Parameter sharing, factorized embeddings
- **DistilBERT**: Destila√ß√£o, 60% menor
- **ELECTRA**: Discriminative pre-training

### 3. GPT (Generative Pre-trained Transformer)

#### 3.1 Arquitetura
- **Decoder-only**: Baseado em decoder do Transformer
- **Autoregressive**: Prediz pr√≥xima palavra
- **Unidirectional**: V√™ apenas contexto anterior

#### 3.2 Evolu√ß√£o da Fam√≠lia GPT

**GPT-1 (2018):**
- 117M par√¢metros
- Unsupervised pre-training + supervised fine-tuning

**GPT-2 (2019):**
- 1.5B par√¢metros
- Zero-shot task transfer
- Controv√©rsia por n√£o libera√ß√£o inicial

**GPT-3 (2020):**
- 175B par√¢metros
- Few-shot learning
- Emergent abilities

**GPT-4 (2023):**
- Multimodal
- Melhor racioc√≠nio
- Mais seguro

#### 3.3 In-Context Learning
Capacidade de aprender tarefas apenas com exemplos no prompt.

**Zero-shot**: Apenas descri√ß√£o da tarefa
**One-shot**: Um exemplo
**Few-shot**: Poucos exemplos

### 4. Transfer Learning em NLP

#### 4.1 Paradigma
1. **Pre-training**: Modelo geral em dados n√£o rotulados
2. **Fine-tuning**: Adaptar para tarefa espec√≠fica
3. **Inference**: Usar modelo adaptado

#### 4.2 Estrat√©gias de Fine-tuning

**Feature-based:**
- Congela pesos do modelo pr√©-treinado
- Usa representa√ß√µes como features
- Adiciona classifier simples

**Fine-tuning:**
- Ajusta todos os pesos
- Learning rate menor que pre-training
- Regulariza√ß√£o forte

**Gradual unfreezing:**
- Descongela camadas gradualmente
- Come√ßar pelas √∫ltimas camadas
- Evita catastrophic forgetting

#### 4.3 Task-specific Heads
- **Classification**: [CLS] ‚Üí Dense ‚Üí Softmax
- **Token classification**: Each token ‚Üí Dense ‚Üí Softmax
- **Question Answering**: Start/End positions
- **Sequence generation**: Language modeling head

### 5. Hugging Face Transformers

#### 5.1 Biblioteca Principal
Padroniza uso de modelos pr√©-treinados.

**Componentes:**
- **Models**: Implementa√ß√µes de arquiteturas
- **Tokenizers**: Pr√©-processamento consistente
- **Pipelines**: Interface high-level
- **Datasets**: Carregamento padronizado

#### 5.2 Pipeline B√°sico
```python
from transformers import pipeline

# An√°lise de sentimentos
classifier = pipeline("sentiment-analysis")
result = classifier("I love this course!")

# Question answering
qa = pipeline("question-answering")
result = qa(question="What is NLP?", context="NLP is...")
```

#### 5.3 Fine-tuning com Trainer
```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

### 6. Otimiza√ß√£o para Produ√ß√£o

#### 6.1 Model Compression

**Pruning:**
- Remove pesos menos importantes
- Structured vs unstructured
- Magnitude-based vs gradient-based

**Quantization:**
- Reduz precis√£o (FP32 ‚Üí INT8)
- Post-training vs quantization-aware training
- Pode manter >95% da performance

**Distillation:**
- Treina modelo menor (student) para imitar maior (teacher)
- Soft targets preserve more information
- DistilBERT, TinyBERT, etc.

#### 6.2 Inference Optimization

**ONNX:**
- Open Neural Network Exchange
- Framework-agnostic
- Hardware-specific optimizations

**TensorRT:**
- NVIDIA optimization library
- Kernel fusion
- Mixed precision

**Dynamic Batching:**
- Agrupa requisi√ß√µes
- Amortiza overhead
- Aumenta throughput

## üõ† Implementa√ß√µes Pr√°ticas

### Fine-tuning BERT
```python
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import TrainingArguments, Trainer

# Carregar modelo e tokenizer
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', 
    num_labels=2
)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokeniza√ß√£o
def tokenize_function(examples):
    return tokenizer(
        examples['text'], 
        truncation=True, 
        padding=True,
        max_length=512
    )

# Fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
```

### GPT para Gera√ß√£o
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Gera√ß√£o de texto
input_text = "The future of AI is"
input_ids = tokenizer.encode(input_text, return_tensors='pt')

output = model.generate(
    input_ids,
    max_length=100,
    num_return_sequences=3,
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)
```

### Custom Model com Transformers
```python
from transformers import BertModel
import torch.nn as nn

class CustomBERT(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(768, num_classes)
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.pooler_output
        pooled = self.dropout(pooled)
        return self.classifier(pooled)
```

## üìä Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Fine-tuning BERT para Classifica√ß√£o
- Dataset de an√°lise de sentimentos
- Comparar com modelos do m√≥dulo anterior
- An√°lise de attention weights

### Exerc√≠cio 2: GPT para Gera√ß√£o de Texto
- Fine-tuning em corpus espec√≠fico
- Diferentes estrat√©gias de decodifica√ß√£o
- Avalia√ß√£o de qualidade (BLEU, perplexity)

### Exerc√≠cio 3: Question Answering com BERT
- Dataset SQuAD em portugu√™s
- Implementar pipeline completo
- Avaliar em diferentes dom√≠nios

### Exerc√≠cio 4: Model Compression
- Aplicar distillation em BERT
- Comparar accuracy vs speed
- Deploy otimizado

## üéØ Pr√≥ximos Passos

No **M√≥dulo 9**, aplicaremos transformers em tarefas avan√ßadas como NER, sumariza√ß√£o e tradu√ß√£o.

---

**Dica**: Execute o notebook `08_transformers_modelos_pretreinados.ipynb` para experimentar com modelos state-of-the-art! 

[‚¨ÖÔ∏è Voltar para o √≠ndice do curso](../README.md) 