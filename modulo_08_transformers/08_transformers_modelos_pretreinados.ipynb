{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# M√≥dulo 8: Transformers e Modelos Pr√©-treinados\n",
        "\n",
        "## üéØ Objetivos\n",
        "- Compreender a arquitetura Transformer\n",
        "- Utilizar modelos pr√©-treinados (BERT, GPT, RoBERTa)\n",
        "- Implementar fine-tuning para tarefas espec√≠ficas\n",
        "- Aplicar transfer learning em NLP\n",
        "- Usar Hugging Face Transformers\n",
        "- Otimizar modelos para produ√ß√£o\n",
        "\n",
        "## üöÄ Modelos State-of-the-Art\n",
        "1. **BERT** - Bidirectional Encoder Representations\n",
        "2. **GPT** - Generative Pre-trained Transformer\n",
        "3. **RoBERTa** - Robustly Optimized BERT Approach\n",
        "4. **DistilBERT** - Destila√ß√£o de conhecimento\n",
        "5. **Fine-tuning** - Adapta√ß√£o para tarefas espec√≠ficas\n",
        "6. **Deployment** - Otimiza√ß√£o para produ√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes para Transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    GPT2Tokenizer, GPT2LMHeadModel,\n",
        "    pipeline, Trainer, TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Datasets\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from datasets.textos_exemplo import *\n",
        "from utils.nlp_utils import *\n",
        "\n",
        "# Verificar GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Usando device: {device}\")\n",
        "print(f\"ü§ó Transformers vers√£o: {transformers.__version__ if 'transformers' in globals() else 'Carregando...'}\")\n",
        "print(\"üöÄ Transformers e modelos pr√©-treinados prontos!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ó 1. Usando BERT Pr√©-treinado para An√°lise de Sentimentos\n",
        "\n",
        "Vamos usar um modelo BERT pr√©-treinado e fazer fine-tuning para nossa tarefa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeiro, vamos usar pipelines pr√©-prontos\n",
        "print(\"üöÄ 1. Usando Pipelines Pr√©-prontos\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Pipeline de an√°lise de sentimentos\n",
        "try:\n",
        "    # Tentar usar modelo em portugu√™s\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"neuralmind/bert-base-portuguese-cased\",\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    print(\"‚úÖ Modelo BERT portugu√™s carregado!\")\n",
        "except:\n",
        "    # Fallback para modelo em ingl√™s\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    print(\"‚úÖ Modelo RoBERTa ingl√™s carregado!\")\n",
        "\n",
        "# Testar com alguns textos\n",
        "textos_teste = [\n",
        "    \"Adorei este produto, muito bom!\",\n",
        "    \"Produto terr√≠vel, n√£o recomendo.\",\n",
        "    \"Qualidade m√©dia, poderia ser melhor.\",\n",
        "    \"Excelente atendimento, muito satisfeito!\",\n",
        "    \"P√©ssima experi√™ncia, muito decepcionante.\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüìù Testando an√°lise de sentimentos:\")\n",
        "for texto in textos_teste:\n",
        "    resultado = sentiment_pipeline(texto)\n",
        "    label = resultado[0]['label']\n",
        "    score = resultado[0]['score']\n",
        "    print(f\"'{texto}'\")\n",
        "    print(f\"  ‚Üí {label} (confian√ßa: {score:.3f})\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ü§ó 2. Usando Transformers Manualmente\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Carregar tokenizer e modelo manualmente\n",
        "model_name = \"distilbert-base-uncased\"  # Modelo menor para demonstra√ß√£o\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, \n",
        "    num_labels=2  # Classifica√ß√£o bin√°ria\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Modelo {model_name} carregado!\")\n",
        "print(f\"üìä Par√¢metros do modelo: {model.num_parameters():,}\")\n",
        "\n",
        "# Preparar dados para fine-tuning\n",
        "reviews_data = [\n",
        "    (\"Excelente produto, recomendo muito!\", 1),\n",
        "    (\"Adorei a qualidade, superou expectativas.\", 1),\n",
        "    (\"Muito bom, chegou r√°pido.\", 1),\n",
        "    (\"Produto incr√≠vel, vale cada centavo!\", 1),\n",
        "    (\"Estou muito satisfeito com a compra.\", 1),\n",
        "    (\"Produto terr√≠vel, n√£o recomendo.\", 0),\n",
        "    (\"Muito ruim, dinheiro jogado fora.\", 0),\n",
        "    (\"Qualidade p√©ssima, chegou quebrado.\", 0),\n",
        "    (\"Horr√≠vel, n√£o serve para nada.\", 0),\n",
        "    (\"Decepcionante, esperava muito mais.\", 0),\n",
        "]\n",
        "\n",
        "texts = [item[0] for item in reviews_data]\n",
        "labels = [item[1] for item in reviews_data]\n",
        "\n",
        "print(f\"\\nüìä Dataset preparado:\")\n",
        "print(f\"Total de exemplos: {len(texts)}\")\n",
        "print(f\"Positivos: {sum(labels)}\")\n",
        "print(f\"Negativos: {len(labels) - sum(labels)}\")\n",
        "\n",
        "# Tokenizar textos\n",
        "print(f\"\\nüî§ Tokenizando textos...\")\n",
        "encoded = tokenizer(\n",
        "    texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Tokeniza√ß√£o completa!\")\n",
        "print(f\"üìä Forma dos input_ids: {encoded['input_ids'].shape}\")\n",
        "print(f\"üìä Forma da attention_mask: {encoded['attention_mask'].shape}\")\n",
        "\n",
        "# Exemplo de tokeniza√ß√£o\n",
        "print(f\"\\nüìù Exemplo de tokeniza√ß√£o:\")\n",
        "exemplo_texto = texts[0]\n",
        "exemplo_tokens = tokenizer.tokenize(exemplo_texto)\n",
        "exemplo_ids = tokenizer.convert_tokens_to_ids(exemplo_tokens)\n",
        "\n",
        "print(f\"Texto: '{exemplo_texto}'\")\n",
        "print(f\"Tokens: {exemplo_tokens}\")\n",
        "print(f\"IDs: {exemplo_ids}\")\n",
        "\n",
        "# Fazer predi√ß√£o com modelo pr√©-treinado (sem fine-tuning)\n",
        "print(f\"\\nüîÆ Predi√ß√µes com modelo pr√©-treinado:\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded)\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_labels = torch.argmax(predictions, dim=-1)\n",
        "\n",
        "for i, (texto, pred_label, probs) in enumerate(zip(texts[:5], predicted_labels[:5], predictions[:5])):\n",
        "    real_label = labels[i]\n",
        "    confidence = torch.max(probs).item()\n",
        "    \n",
        "    print(f\"'{texto[:50]}...'\")\n",
        "    print(f\"  Real: {real_label}, Predito: {pred_label.item()}, Confian√ßa: {confidence:.3f}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ 3. Comparando com M√©todos Tradicionais\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Comparar com TF-IDF + Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Preparar dados para sklearn\n",
        "X_traditional = texts\n",
        "y_traditional = labels\n",
        "\n",
        "# TF-IDF + Logistic Regression\n",
        "tfidf = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = tfidf.fit_transform(X_traditional)\n",
        "\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_scores = cross_val_score(lr_model, X_tfidf, y_traditional, cv=3, scoring='accuracy')\n",
        "\n",
        "print(f\"üìä Compara√ß√£o de Performance:\")\n",
        "print(f\"TF-IDF + Logistic Regression: {lr_scores.mean():.3f} (¬±{lr_scores.std():.3f})\")\n",
        "\n",
        "# Calcular accuracy do modelo pr√©-treinado\n",
        "transformer_accuracy = accuracy_score(labels, predicted_labels.numpy())\n",
        "print(f\"BERT pr√©-treinado (sem fine-tuning): {transformer_accuracy:.3f}\")\n",
        "\n",
        "print(f\"\\nüí° Observa√ß√µes:\")\n",
        "print(f\"- O modelo BERT foi treinado em dados gerais, n√£o espec√≠ficos para reviews\")\n",
        "print(f\"- Com fine-tuning, a performance do BERT seria significativamente melhor\")\n",
        "print(f\"- Para datasets pequenos, m√©todos tradicionais podem ser mais eficientes\")\n",
        "\n",
        "# Demonstrar capacidades do modelo\n",
        "print(f\"\\nüß™ Testando capacidades do modelo:\")\n",
        "novos_textos = [\n",
        "    \"Este produto mudou minha vida!\",\n",
        "    \"Pior compra que j√° fiz na vida.\",\n",
        "    \"Produto ok, nada demais.\",\n",
        "    \"Simplesmente fant√°stico, recomendo!\"\n",
        "]\n",
        "\n",
        "# Tokenizar novos textos\n",
        "new_encoded = tokenizer(\n",
        "    novos_textos,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Predizer\n",
        "with torch.no_grad():\n",
        "    new_outputs = model(**new_encoded)\n",
        "    new_predictions = torch.nn.functional.softmax(new_outputs.logits, dim=-1)\n",
        "    new_labels = torch.argmax(new_predictions, dim=-1)\n",
        "\n",
        "print(f\"\\nüîÆ Predi√ß√µes para novos textos:\")\n",
        "for texto, pred, probs in zip(novos_textos, new_labels, new_predictions):\n",
        "    sentiment = \"Positivo\" if pred.item() == 1 else \"Negativo\"\n",
        "    confidence = torch.max(probs).item()\n",
        "    \n",
        "    print(f\"'{texto}'\")\n",
        "    print(f\"  ‚Üí {sentiment} (confian√ßa: {confidence:.3f})\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
