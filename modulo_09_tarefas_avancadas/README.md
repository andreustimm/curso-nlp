[‚¨ÖÔ∏è Voltar](../README.md)

# M√≥dulo 9: Tarefas Avan√ßadas de NLP

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Implementar sistemas de Question Answering
- Construir modelos de sumariza√ß√£o autom√°tica
- Desenvolver sistemas de tradu√ß√£o neural
- Aplicar modelos em an√°lise de di√°logo
- Implementar detec√ß√£o de t√≥picos com LDA
- Criar sistemas de recomenda√ß√£o baseados em texto

## üìö Conte√∫do Te√≥rico

### 1. Question Answering (QA)

#### 1.1 Tipos de QA Systems

**Extractive QA:**
- Resposta √© span do texto
- SQuAD, MS MARCO
- BERT-based models

**Generative QA:**
- Resposta √© gerada
- Pode sintetizar informa√ß√£o
- T5, BART, GPT

**Open-domain QA:**
- Busca em grande corpus
- Retrieve + Read pipeline
- DPR (Dense Passage Retrieval)

#### 1.2 Arquiteturas para QA

**BERT for QA:**
```
[CLS] question [SEP] context [SEP]
‚Üì
BERT Encoder
‚Üì
Start/End Position Prediction
```

**T5 for QA:**
```
Input: "question: What is NLP? context: NLP is..."
Output: "Natural Language Processing"
```

#### 1.3 Metrics de Avalia√ß√£o
- **Exact Match (EM)**: Resposta exata
- **F1 Score**: Overlap de tokens
- **BLEU**: Para respostas generativas
- **ROUGE**: Para sumariza√ß√£o

### 2. Sumariza√ß√£o Autom√°tica

#### 2.1 Tipos de Sumariza√ß√£o

**Extractive:**
- Seleciona senten√ßas importantes
- Preserva texto original
- TextRank, BERT-based

**Abstractive:**
- Gera novo texto
- Pode parafrasear
- Seq2Seq, Transformer

**Single vs Multi-document:**
- Um documento vs m√∫ltiplos
- Complexidade de fus√£o
- Redund√¢ncia e contradi√ß√µes

#### 2.2 Algoritmos Extractivos

**TextRank:**
- PageRank para senten√ßas
- Grafo de similaridade
- N√£o supervisionado

**BERT-based:**
- BERT embeddings
- Clustering ou ranking
- Fine-tuning possible

#### 2.3 Modelos Abstractivos

**BART:**
- Denoising autoencoder
- Encoder-decoder
- State-of-the-art performance

**T5:**
- Text-to-Text Transfer Transformer
- "summarize: [text]"
- Unified framework

**PEGASUS:**
- Pr√©-treinado especificamente para sumariza√ß√£o
- Gap sentence generation
- Strong baseline

### 3. Tradu√ß√£o Neural (NMT)

#### 3.1 Evolu√ß√£o da Tradu√ß√£o

**Statistical MT (SMT):**
- Phrase-based translation
- Language models
- Alignment models

**Neural MT (NMT):**
- End-to-end neural networks
- Better fluency
- Context awareness

#### 3.2 Arquiteturas NMT

**Seq2Seq with Attention:**
- Encoder-Decoder RNN/LSTM
- Attention mechanism
- Alinhamento autom√°tico

**Transformer NMT:**
- Self-attention
- Paraleliza√ß√£o
- State-of-the-art

#### 3.3 Desafios em NMT
- **Low-resource languages**
- **Domain adaptation**
- **Rare words (OOV)**
- **Long sentences**
- **Evaluation metrics**

### 4. Topic Modeling

#### 4.1 Latent Dirichlet Allocation (LDA)

**Intui√ß√£o:**
- Documentos s√£o misturas de t√≥picos
- T√≥picos s√£o distribui√ß√µes de palavras
- Infer√™ncia Bayesiana

**Modelo Generativo:**
```
Para cada documento d:
  Œ∏_d ~ Dirichlet(Œ±)  # Distribui√ß√£o de t√≥picos
  Para cada palavra w:
    z ~ Multinomial(Œ∏_d)  # Escolher t√≥pico
    w ~ Multinomial(œÜ_z)  # Gerar palavra
```

#### 4.2 Algoritmos de Infer√™ncia

**Gibbs Sampling:**
- MCMC method
- Sampling z_i dado z_{-i}
- Converg√™ncia lenta mas precisa

**Variational Inference:**
- Aproxima√ß√£o determin√≠stica
- Mais r√°pido que Gibbs
- Usado no scikit-learn

#### 4.3 Avalia√ß√£o de T√≥picos
- **Perplexity**: Likelihood dos dados
- **Coherence**: Coer√™ncia sem√¢ntica
- **Topic diversity**: Variedade de t√≥picos
- **Human evaluation**: Julgamento humano

### 5. An√°lise de Di√°logo

#### 5.1 Componentes de Chatbots

**Natural Language Understanding (NLU):**
- Intent classification
- Entity extraction
- Slot filling

**Dialogue Management:**
- State tracking
- Policy learning
- Context maintenance

**Natural Language Generation (NLG):**
- Response generation
- Template-based vs neural
- Personality and style

#### 5.2 Intent Classification
```
User: "Book a flight to Paris"
Intent: book_flight
Entities: {destination: "Paris"}
```

#### 5.3 Response Generation

**Template-based:**
- Rule-based responses
- Slot filling
- Deterministic

**Neural Generation:**
- Seq2Seq models
- Personality modeling
- Contextual responses

### 6. Information Extraction

#### 6.1 Named Entity Recognition (NER)

**Nested NER:**
- Entidades aninhadas
- "Apple Inc. CEO Tim Cook"
- Estruturas complexas

**Few-shot NER:**
- Poucos exemplos
- Meta-learning
- Transfer learning

#### 6.2 Relation Extraction
- Identificar rela√ß√µes entre entidades
- "Tim Cook works for Apple"
- Distant supervision

#### 6.3 Event Extraction
- Detectar eventos em texto
- Participantes, tempo, local
- ACE datasets

## üõ† Implementa√ß√µes Pr√°ticas

### Question Answering com BERT
```python
from transformers import BertForQuestionAnswering, BertTokenizer

model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def answer_question(question, context):
    inputs = tokenizer(question, context, return_tensors='pt', 
                      truncation=True, padding=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    start_scores = outputs.start_logits
    end_scores = outputs.end_logits
    
    start_idx = torch.argmax(start_scores)
    end_idx = torch.argmax(end_scores)
    
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+1])
    
    return answer
```

### Sumariza√ß√£o com BART
```python
from transformers import BartTokenizer, BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

def summarize_text(text, max_length=150):
    inputs = tokenizer(text, return_tensors='pt', 
                      max_length=1024, truncation=True)
    
    summary_ids = model.generate(
        inputs['input_ids'],
        max_length=max_length,
        min_length=30,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
```

### Topic Modeling com LDA
```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Preparar dados
vectorizer = CountVectorizer(max_features=1000, stop_words='english')
doc_term_matrix = vectorizer.fit_transform(documents)

# Treinar LDA
lda = LatentDirichletAllocation(
    n_components=10,
    random_state=42,
    max_iter=100
)
lda.fit(doc_term_matrix)

# Extrair t√≥picos
feature_names = vectorizer.get_feature_names_out()

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[-no_top_words:]]
        print(f"Topic {topic_idx}: {' '.join(top_words)}")
```

### Tradu√ß√£o com MarianMT
```python
from transformers import MarianMTModel, MarianTokenizer

model_name = 'Helsinki-NLP/opus-mt-en-pt'
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

def translate_text(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True)
    translated = model.generate(**inputs)
    return tokenizer.decode(translated[0], skip_special_tokens=True)
```

## üìä Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Sistema de QA
- Implementar QA extractivo e generativo
- Avaliar em dataset portugu√™s
- Interface web simples

### Exerc√≠cio 2: Sumarizador de Not√≠cias
- Comparar m√©todos extractivos e abstractivos
- Avaliar com ROUGE
- Pipeline end-to-end

### Exerc√≠cio 3: An√°lise de T√≥picos
- LDA em corpus de documentos
- Visualiza√ß√£o de t√≥picos
- Evolu√ß√£o temporal

### Exerc√≠cio 4: Chatbot Simples
- Intent classification
- Entity extraction
- Response generation

### Exerc√≠cio 5: Sistema de Tradu√ß√£o
- Fine-tuning para dom√≠nio espec√≠fico
- Avalia√ß√£o autom√°tica e humana
- An√°lise de erros

## üéØ Pr√≥ximos Passos

No **M√≥dulo 10**, integraremos tudo em projetos pr√°ticos completos para portf√≥lio.

---

**Dica**: Execute o notebook `09_tarefas_avancadas_nlp.ipynb` para implementar sistemas completos! 

[‚¨ÖÔ∏è Voltar](../README.md) 