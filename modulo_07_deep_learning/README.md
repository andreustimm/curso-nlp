[‚¨ÖÔ∏è **Voltar**](../README.md)

# M√≥dulo 7: Deep Learning para NLP

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Implementar redes neurais feedforward para texto
- Construir e treinar RNNs, LSTMs e GRUs
- Aplicar CNNs para classifica√ß√£o de texto
- Compreender e implementar attention mechanisms
- Otimizar arquiteturas neurais para NLP
- Debugar e melhorar modelos de deep learning

## üìö Conte√∫do Te√≥rico

### 1. Redes Neurais Feedforward

#### 1.1 Conceitos B√°sicos
- **Perceptron**: Unidade b√°sica
- **Multi-layer Perceptron**: M√∫ltiplas camadas
- **Backpropagation**: Treinamento via gradiente
- **Universal approximation**: Capacidade de aproximar qualquer fun√ß√£o

#### 1.2 Aplica√ß√£o em Texto
```python
# Arquitetura simples
Input (TF-IDF) ‚Üí Hidden Layer ‚Üí ReLU ‚Üí Output Layer ‚Üí Softmax
```

**Vantagens:**
- Simples de implementar
- Baseline forte
- R√°pido treinamento

**Limita√ß√µes:**
- Ignora ordem das palavras
- Features fixas
- Sem mem√≥ria de longo prazo

### 2. Redes Neurais Recorrentes (RNN)

#### 2.1 Motiva√ß√£o
Texto √© sequencial. RNNs processam sequ√™ncias mantendo estado interno.

#### 2.2 Arquitetura RNN
```
h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
y_t = W_hy * h_t + b_y
```

**Componentes:**
- **Estado oculto**: h_t (mem√≥ria)
- **Pesos recorrentes**: W_hh
- **Pesos de entrada**: W_xh

#### 2.3 Problemas das RNNs
- **Vanishing gradients**: Gradientes diminuem exponencialmente
- **Exploding gradients**: Gradientes explodem
- **Depend√™ncias longas**: Dificuldade em capturar contexto distante

### 3. Long Short-Term Memory (LSTM)

#### 3.1 Arquitetura LSTM
Resolve problemas de RNN atrav√©s de gates:

**Forget Gate:**
f_t = œÉ(W_f * [h_{t-1}, x_t] + b_f)

**Input Gate:**
i_t = œÉ(W_i * [h_{t-1}, x_t] + b_i)
CÃÉ_t = tanh(W_C * [h_{t-1}, x_t] + b_C)

**Cell State Update:**
C_t = f_t * C_{t-1} + i_t * CÃÉ_t

**Output Gate:**
o_t = œÉ(W_o * [h_{t-1}, x_t] + b_o)
h_t = o_t * tanh(C_t)

#### 3.2 Vantagens
- Captura depend√™ncias longas
- Controla fluxo de informa√ß√£o
- Gradientes mais est√°veis
- Vers√°til para muitas tarefas

### 4. Gated Recurrent Unit (GRU)

#### 4.1 Simplifica√ß√£o da LSTM
- Menos par√¢metros que LSTM
- Combina forget e input gates
- Performance similar √† LSTM
- Treinamento mais r√°pido

#### 4.2 F√≥rmulas GRU
```
r_t = œÉ(W_r * [h_{t-1}, x_t])    # Reset gate
z_t = œÉ(W_z * [h_{t-1}, x_t])    # Update gate
hÃÉ_t = tanh(W * [r_t * h_{t-1}, x_t])
h_t = (1 - z_t) * h_{t-1} + z_t * hÃÉ_t
```

### 5. Redes Neurais Convolucionais (CNN)

#### 5.1 CNNs para Texto
Aplicam filtros convolucionais sobre embeddings de palavras.

**Arquitetura:**
```
Input Embeddings ‚Üí Conv1D ‚Üí ReLU ‚Üí MaxPool ‚Üí Dense ‚Üí Softmax
```

#### 5.2 Vantagens
- Detecta n-gramas automaticamente
- Translation invariant
- Paraleliza√ß√£o eficiente
- Bom para classifica√ß√£o

#### 5.3 Hiperpar√¢metros
- **Filter sizes**: [2,3,4,5] para capturar diferentes n-gramas
- **Number of filters**: 100-200 por tamanho
- **Pooling**: Max pooling mais comum

### 6. Attention Mechanisms

#### 6.1 Motiva√ß√£o
RNNs t√™m bottleneck no √∫ltimo estado. Attention permite acesso a todos estados.

#### 6.2 Attention B√°sico
```
e_ij = a(s_{i-1}, h_j)           # Energy/score
Œ±_ij = softmax(e_ij)             # Attention weights
c_i = Œ£ Œ±_ij * h_j               # Context vector
```

#### 6.3 Tipos de Attention
- **Additive (Bahdanau)**: MLP para computar scores
- **Multiplicative (Luong)**: Produto escalar
- **Self-attention**: Query, key, value da mesma sequ√™ncia

#### 6.4 Multi-Head Attention
- M√∫ltiplas "cabe√ßas" de attention
- Capturam diferentes tipos de rela√ß√µes
- Base dos Transformers

### 7. Arquiteturas Avan√ßadas

#### 7.1 Bidirectional RNN/LSTM
- Processa sequ√™ncia nas duas dire√ß√µes
- Combina informa√ß√£o passada e futura
- Melhor para tarefas onde contexto completo est√° dispon√≠vel

#### 7.2 Sequence-to-Sequence (Seq2Seq)
- Encoder-Decoder architecture
- Encoder: RNN que processa entrada
- Decoder: RNN que gera sa√≠da
- Aplica√ß√µes: tradu√ß√£o, sumariza√ß√£o

#### 7.3 Attention-based Seq2Seq
- Adiciona attention ao Seq2Seq
- Resolver bottleneck do encoder
- Permite alinhamento entrada-sa√≠da

## üõ† Implementa√ß√µes Pr√°ticas

### LSTM com TensorFlow/Keras
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding

model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

### CNN para Texto
```python
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D

model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])
```

### Attention Layer
```python
import tensorflow as tf

class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)
        
    def call(self, hidden_states):
        # hidden_states: (batch_size, time_steps, features)
        score = self.V(tf.nn.tanh(self.W(hidden_states)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * hidden_states
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights
```

## üìä Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Compara√ß√£o de Arquiteturas
- Implementar MLP, RNN, LSTM, GRU, CNN
- Avaliar em dataset de classifica√ß√£o
- Analisar tempo de treinamento vs performance

### Exerc√≠cio 2: An√°lise de Sentimentos com LSTM
- Dataset de reviews
- Bidirectional LSTM
- Attention visualization

### Exerc√≠cio 3: CNN para Classifica√ß√£o de Texto
- M√∫ltiplos filter sizes
- Regulariza√ß√£o (dropout, batch norm)
- Comparar com RNNs

### Exerc√≠cio 4: Seq2Seq com Attention
- Tarefa de sumariza√ß√£o
- Implementar attention mechanism
- Visualizar attention weights

## üéØ Pr√≥ximos Passos

No **M√≥dulo 8**, exploraremos Transformers e modelos pr√©-treinados como BERT e GPT.

---

**Dica**: Execute o notebook `07_deep_learning_nlp.ipynb` para implementar todas as arquiteturas! 

[‚¨ÖÔ∏è **Voltar**](../README.md) 