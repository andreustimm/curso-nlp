{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# M√≥dulo 7: Deep Learning para NLP\n",
        "\n",
        "## üéØ Objetivos\n",
        "- Implementar redes neurais feedforward para texto\n",
        "- Construir RNNs, LSTMs e GRUs\n",
        "- Aplicar CNNs em classifica√ß√£o de texto\n",
        "- Implementar attention mechanisms\n",
        "- Comparar arquiteturas neurais\n",
        "- Debugar e otimizar modelos\n",
        "\n",
        "## üß† Arquiteturas Neurais\n",
        "1. **Feedforward Networks** - MLPs para texto\n",
        "2. **RNNs** - Redes recorrentes b√°sicas\n",
        "3. **LSTM/GRU** - Mem√≥ria de longo prazo\n",
        "4. **CNNs** - Redes convolucionais 1D\n",
        "5. **Attention** - Mecanismos de aten√ß√£o\n",
        "6. **Seq2Seq** - Encoder-Decoder com aten√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes para deep learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Dense, LSTM, GRU, SimpleRNN, Embedding, \n",
        "                                   Conv1D, MaxPooling1D, GlobalMaxPooling1D,\n",
        "                                   Dropout, BatchNormalization, Attention,\n",
        "                                   Input, Bidirectional, TimeDistributed)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Scikit-learn para compara√ß√£o\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Datasets\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from datasets.textos_exemplo import *\n",
        "from utils.nlp_utils import *\n",
        "\n",
        "# Configurar TensorFlow\n",
        "tf.random.set_seed(42)\n",
        "print(f\"üß† TensorFlow {tf.__version__} carregado!\")\n",
        "print(f\"GPU dispon√≠vel: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(\"üß† Deep Learning para NLP pronto!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß† 1. Implementa√ß√£o de LSTM para Classifica√ß√£o de Texto\n",
        "\n",
        "Vamos construir uma rede LSTM completa para classifica√ß√£o de sentimentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar dados para deep learning\n",
        "reviews_positivos = [\n",
        "    \"Excelente produto, recomendo muito!\",\n",
        "    \"Adorei a qualidade, superou minhas expectativas.\",\n",
        "    \"Muito bom, chegou r√°pido e bem embalado.\",\n",
        "    \"Produto incr√≠vel, vale cada centavo!\",\n",
        "    \"Estou muito satisfeito com a compra.\",\n",
        "    \"Qualidade excepcional, comprarei novamente.\",\n",
        "    \"Fant√°stico! Melhor que imaginava.\",\n",
        "    \"Produto maravilhoso, entrega r√°pida.\",\n",
        "    \"Simplesmente perfeito, recomendo a todos.\",\n",
        "    \"Excelente custo-benef√≠cio, muito bom produto.\",\n",
        "    \"Adorei o produto, muito √∫til no dia a dia.\",\n",
        "    \"Superou todas as expectativas, incr√≠vel!\",\n",
        "    \"Produto de alta qualidade, vale a pena.\",\n",
        "    \"Muito satisfeito, chegou antes do prazo.\",\n",
        "    \"Excelente atendimento e produto top!\"\n",
        "]\n",
        "\n",
        "reviews_negativos = [\n",
        "    \"Produto terr√≠vel, n√£o recomendo.\",\n",
        "    \"Muito ruim, dinheiro jogado fora.\",\n",
        "    \"Qualidade p√©ssima, chegou quebrado.\",\n",
        "    \"Horr√≠vel, n√£o serve para nada.\",\n",
        "    \"Decepcionante, esperava muito mais.\",\n",
        "    \"Produto de baixa qualidade, muito caro.\",\n",
        "    \"N√£o gostei, vou devolver.\",\n",
        "    \"P√©ssimo atendimento e produto ruim.\",\n",
        "    \"N√£o vale o pre√ßo, muito caro para pouca qualidade.\",\n",
        "    \"Produto defeituoso, n√£o funcionou.\",\n",
        "    \"Muito ruim, n√£o recomendo a ningu√©m.\",\n",
        "    \"P√©ssima qualidade, chegou danificado.\",\n",
        "    \"Produto horr√≠vel, perda de tempo.\",\n",
        "    \"N√£o gostei nada, muito decepcionante.\",\n",
        "    \"Qualidade inferior, n√£o vale o dinheiro.\"\n",
        "]\n",
        "\n",
        "# Criar dataset\n",
        "texts = reviews_positivos + reviews_negativos\n",
        "labels = [1] * len(reviews_positivos) + [0] * len(reviews_negativos)\n",
        "\n",
        "print(f\"üìä Dataset para Deep Learning:\")\n",
        "print(f\"Total de textos: {len(texts)}\")\n",
        "print(f\"Positivos: {sum(labels)}\")\n",
        "print(f\"Negativos: {len(labels) - sum(labels)}\")\n",
        "\n",
        "# Configura√ß√µes\n",
        "MAX_WORDS = 1000\n",
        "MAX_LEN = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Tokeniza√ß√£o e prepara√ß√£o dos dados\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Converter textos para sequ√™ncias\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding das sequ√™ncias\n",
        "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "y = np.array(labels)\n",
        "\n",
        "print(f\"\\nüî¢ Dados processados:\")\n",
        "print(f\"Forma de X: {X.shape}\")\n",
        "print(f\"Forma de y: {y.shape}\")\n",
        "print(f\"Vocabul√°rio: {len(tokenizer.word_index)} palavras\")\n",
        "\n",
        "# Exemplo de sequ√™ncia processada\n",
        "print(f\"\\nüìù Exemplo de processamento:\")\n",
        "print(f\"Texto original: '{texts[0]}'\")\n",
        "print(f\"Sequ√™ncia: {sequences[0]}\")\n",
        "print(f\"Padded: {X[0]}\")\n",
        "\n",
        "# Divis√£o treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nüîÄ Divis√£o dos dados:\")\n",
        "print(f\"Treino: {X_train.shape}\")\n",
        "print(f\"Teste: {X_test.shape}\")\n",
        "\n",
        "# Fun√ß√£o para criar modelo LSTM\n",
        "def create_lstm_model(vocab_size, embedding_dim, max_length, lstm_units=64):\n",
        "    \"\"\"Cria modelo LSTM para classifica√ß√£o bin√°ria\"\"\"\n",
        "    \n",
        "    model = Sequential([\n",
        "        # Camada de embedding\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        \n",
        "        # Dropout para regulariza√ß√£o\n",
        "        Dropout(0.3),\n",
        "        \n",
        "        # Camada LSTM\n",
        "        LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3),\n",
        "        \n",
        "        # Camada densa\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        \n",
        "        # Camada de sa√≠da\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Criar modelo\n",
        "model = create_lstm_model(\n",
        "    vocab_size=MAX_WORDS,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    max_length=MAX_LEN,\n",
        "    lstm_units=64\n",
        ")\n",
        "\n",
        "# Compilar modelo\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Mostrar arquitetura\n",
        "print(f\"\\nüèó Arquitetura do modelo:\")\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=0.0001\n",
        ")\n",
        "\n",
        "# Treinar modelo\n",
        "print(f\"\\nüöÄ Treinando modelo LSTM...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Avaliar modelo\n",
        "print(f\"\\nüìä Avalia√ß√£o do modelo:\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
        "print(f\"Loss no teste: {test_loss:.4f}\")\n",
        "\n",
        "# Predi√ß√µes\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Relat√≥rio de classifica√ß√£o\n",
        "print(f\"\\nüìã Relat√≥rio de classifica√ß√£o:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n",
        "\n",
        "# Visualizar hist√≥rico de treinamento\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy\n",
        "ax1.plot(history.history['accuracy'], label='Treino')\n",
        "ax1.plot(history.history['val_accuracy'], label='Valida√ß√£o')\n",
        "ax1.set_title('Acur√°cia do Modelo')\n",
        "ax1.set_xlabel('√âpoca')\n",
        "ax1.set_ylabel('Acur√°cia')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "ax2.plot(history.history['loss'], label='Treino')\n",
        "ax2.plot(history.history['val_loss'], label='Valida√ß√£o')\n",
        "ax2.set_title('Loss do Modelo')\n",
        "ax2.set_xlabel('√âpoca')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Testar com novos textos\n",
        "novos_textos = [\n",
        "    \"Este produto √© fant√°stico, recomendo!\",\n",
        "    \"Muito ruim, n√£o gostei nada.\",\n",
        "    \"Qualidade m√©dia, poderia ser melhor.\",\n",
        "    \"Excelente compra, muito satisfeito!\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüß™ Testando com novos textos:\")\n",
        "for texto in novos_textos:\n",
        "    # Processar texto\n",
        "    seq = tokenizer.texts_to_sequences([texto])\n",
        "    padded = pad_sequences(seq, maxlen=MAX_LEN, padding='post')\n",
        "    \n",
        "    # Predizer\n",
        "    pred_proba = model.predict(padded, verbose=0)[0][0]\n",
        "    pred_label = \"Positivo\" if pred_proba > 0.5 else \"Negativo\"\n",
        "    \n",
        "    print(f\"'{texto}'\")\n",
        "    print(f\"  ‚Üí {pred_label} (confian√ßa: {pred_proba:.3f})\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
