[‚¨ÖÔ∏è **Voltar**](../README.md)

# M√≥dulo 4: Representa√ß√£o de Texto

## üéØ Objetivos do M√≥dulo

Ao final deste m√≥dulo, voc√™ ser√° capaz de:
- Implementar Bag of Words (BoW) e suas varia√ß√µes
- Calcular e interpretar TF-IDF
- Treinar e utilizar word embeddings (Word2Vec, GloVe, FastText)
- Avaliar qualidade de representa√ß√µes vetoriais
- Aplicar redu√ß√£o de dimensionalidade em dados textuais
- Escolher a representa√ß√£o adequada para cada tarefa

## üìö Conte√∫do Te√≥rico

### 1. Fundamentos da Representa√ß√£o Vetorial

#### 1.1 Por que Vetorizar Texto?
Algoritmos de machine learning trabalham com n√∫meros, n√£o com texto. A vetoriza√ß√£o converte texto em representa√ß√µes num√©ricas preservando informa√ß√£o sem√¢ntica.

**Desafios:**
- **Esparsidade**: Maioria dos valores s√£o zero
- **Dimensionalidade**: Vocabul√°rios grandes = vetores enormes
- **Sem√¢ntica**: Capturar significado al√©m de sintaxe
- **Efici√™ncia**: Processamento r√°pido para grandes corpus

#### 1.2 Tipos de Representa√ß√£o
- **One-hot encoding**: Cada palavra = vetor bin√°rio
- **Contagem**: Frequ√™ncia de palavras
- **TF-IDF**: Frequ√™ncia ponderada por raridade
- **Embeddings**: Representa√ß√µes densas aprendidas

### 2. Bag of Words (BoW)

#### 2.1 Conceito B√°sico
Representa documento como "saco de palavras", ignorando ordem e estrutura.

**Caracter√≠sticas:**
- **Independ√™ncia posicional**: Ordem n√£o importa
- **Esparsidade**: Muitos zeros
- **Interpretabilidade**: F√°cil de entender
- **Baseline**: Boa linha de base para compara√ß√£o

#### 2.2 Varia√ß√µes do BoW

**Binary BoW:**
- 1 se palavra presente, 0 caso contr√°rio
- Ignora frequ√™ncia
- √ötil para classifica√ß√£o de t√≥picos

**Count BoW:**
- Frequ√™ncia absoluta de cada palavra
- Considera repeti√ß√µes
- Pode dar peso excessivo a palavras comuns

**Normalized BoW:**
- Frequ√™ncias normalizadas por comprimento
- Evita vi√©s por tamanho de documento
- F√≥rmulas: L1, L2, max normalization

#### 2.3 Implementa√ß√£o
```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(
    max_features=5000,      # Vocabul√°rio m√°ximo
    ngram_range=(1, 2),     # Uni e bigramas
    stop_words='english',   # Remover stopwords
    min_df=2,               # Frequ√™ncia m√≠nima
    max_df=0.95             # Frequ√™ncia m√°xima
)
```

### 3. TF-IDF (Term Frequency - Inverse Document Frequency)

#### 3.1 Motiva√ß√£o
BoW trata todas as palavras igualmente. TF-IDF pondera palavras por sua import√¢ncia no corpus.

**Intui√ß√£o:**
- Palavras frequentes no documento s√£o importantes (TF)
- Palavras raras no corpus s√£o distintivas (IDF)
- Combine ambas para medir relev√¢ncia

#### 3.2 F√≥rmulas Matem√°ticas

**Term Frequency (TF):**
- TF(t,d) = (# de vezes que t aparece em d) / (# total de palavras em d)
- Log normalization: 1 + log(tf)
- Binary: 1 se presente, 0 caso contr√°rio

**Inverse Document Frequency (IDF):**
- IDF(t) = log(N / df(t))
- N = n√∫mero total de documentos
- df(t) = n√∫mero de documentos contendo termo t

**TF-IDF Score:**
- TF-IDF(t,d) = TF(t,d) √ó IDF(t)
- Smoothed IDF: log(N / (1 + df(t))) + 1

#### 3.3 Interpreta√ß√£o
- **Valores altos**: Palavras frequentes no documento, raras no corpus
- **Valores baixos**: Palavras comuns ou ausentes
- **Zero**: Palavra n√£o aparece no documento
- **Stopwords**: Geralmente t√™m IDF baixo

#### 3.4 Vantagens e Limita√ß√µes

**Vantagens:**
- Reduz peso de palavras comuns
- Destaca termos distintivos
- Funciona bem para busca e classifica√ß√£o
- Interpret√°vel e eficiente

**Limita√ß√µes:**
- Ainda ignora ordem das palavras
- N√£o captura rela√ß√µes sem√¢nticas
- Esparsidade permanece
- Sens√≠vel ao tamanho do corpus

### 4. Word Embeddings

#### 4.1 Conceito
Representa√ß√µes densas de baixa dimensionalidade que capturam rela√ß√µes sem√¢nticas.

**Hip√≥tese distribucional**: "Palavras que aparecem em contextos similares t√™m significados similares"

**Caracter√≠sticas:**
- **Densas**: Poucos zeros, muita informa√ß√£o
- **Baixa dimensionalidade**: 50-300 dimens√µes
- **Sem√¢ntica**: Capturam rela√ß√µes de significado
- **Aprendizagem**: Extra√≠das de grandes corpus

#### 4.2 Word2Vec

**Arquiteturas:**
- **CBOW (Continuous Bag of Words)**: Prediz palavra central dado contexto
- **Skip-gram**: Prediz contexto dada palavra central

**Hiperpar√¢metros:**
- **vector_size**: Dimensionalidade (100-300)
- **window**: Tamanho da janela de contexto
- **min_count**: Frequ√™ncia m√≠nima das palavras
- **workers**: Paraleliza√ß√£o
- **sg**: 0=CBOW, 1=Skip-gram

**Propriedades Matem√°ticas:**
- rei - homem + mulher ‚âà rainha
- Paris - Fran√ßa + It√°lia ‚âà Roma
- walking - walk + swim ‚âà swimming

#### 4.3 GloVe (Global Vectors)
Combina estat√≠sticas globais (matriz de co-ocorr√™ncia) com aprendizado local.

**Vantagens sobre Word2Vec:**
- Usa estat√≠sticas globais do corpus
- Treinamento mais est√°vel
- Melhor performance em tarefas de analogia

#### 4.4 FastText
Extens√£o do Word2Vec que considera subpalavras (character n-grams).

**Vantagens:**
- Lida com palavras fora do vocabul√°rio (OOV)
- √ötil para idiomas com morfologia rica
- Compartilha informa√ß√£o entre palavras similares

### 5. Redu√ß√£o de Dimensionalidade

#### 5.1 Por que Reduzir Dimens√µes?
- **Visualiza√ß√£o**: Plotar em 2D/3D
- **Efici√™ncia**: Menos par√¢metros para treinar
- **Ru√≠do**: Remover dimens√µes irrelevantes
- **Storage**: Economizar mem√≥ria

#### 5.2 T√©cnicas Lineares

**PCA (Principal Component Analysis):**
- Encontra dire√ß√µes de maior vari√¢ncia
- Componentes s√£o combina√ß√µes lineares de features
- Preserva vari√¢ncia global

**SVD (Singular Value Decomposition):**
- Factoriza√ß√£o de matriz
- Base para Latent Semantic Analysis (LSA)
- Identifica t√≥picos latentes

#### 5.3 T√©cnicas N√£o-lineares

**t-SNE (t-Distributed Stochastic Neighbor Embedding):**
- Preserva estrutura local
- Excelente para visualiza√ß√£o
- N√£o preserva dist√¢ncias globais

**UMAP (Uniform Manifold Approximation):**
- Mais r√°pido que t-SNE
- Preserva melhor estrutura global
- Bom para visualiza√ß√£o e pr√©-processamento

### 6. Avalia√ß√£o de Representa√ß√µes

#### 6.1 M√©tricas Intr√≠nsecas

**Analogias:**
- a : b :: c : ?
- Teste: rei : homem :: rainha : mulher
- Datasets: Google analogies, BATS

**Similaridade:**
- Correla√ß√£o com julgamentos humanos
- Datasets: SimLex-999, WordSim-353
- M√©tricas: Spearman correlation

**Outlier Detection:**
- Identificar palavra que n√£o pertence ao grupo
- Exemplo: [gato, cachorro, carro, gato]

#### 6.2 M√©tricas Extr√≠nsecas
- **Classifica√ß√£o de texto**: Accuracy, F1-score
- **An√°lise de sentimentos**: Correla√ß√£o com labels
- **NER**: Precision, Recall, F1
- **Parsing**: Attachment accuracy

#### 6.3 An√°lise Qualitativa
- **Vizinhos mais pr√≥ximos**: Palavras similares
- **Visualiza√ß√£o**: t-SNE, UMAP plots
- **Proje√ß√µes**: Opera√ß√µes matem√°ticas
- **Bias detection**: Preconceitos nos embeddings

## üõ† Implementa√ß√µes Pr√°ticas

### Bag of Words com Scikit-learn
```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# BoW simples
bow_vectorizer = CountVectorizer(max_features=1000)
bow_matrix = bow_vectorizer.fit_transform(corpus)

# TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2))
tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)
```

### Word2Vec com Gensim
```python
from gensim.models import Word2Vec

# Treinar modelo
model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=100,
    window=5,
    min_count=5,
    workers=4,
    sg=1  # Skip-gram
)

# Usar modelo
vector = model.wv['palavra']
similares = model.wv.most_similar('palavra', topn=10)
```

### FastText
```python
from gensim.models import FastText

model = FastText(
    sentences=tokenized_corpus,
    vector_size=100,
    window=5,
    min_count=5,
    workers=4,
    sg=1
)

# Vantagem: lida com OOV
vector_oov = model.wv['palavrainexistente']
```

## üìä Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Compara√ß√£o BoW vs TF-IDF
- Implementar ambas representa√ß√µes
- Comparar esparsidade e performance
- Analisar palavras com maior peso

### Exerc√≠cio 2: Treinamento de Word2Vec
- Treinar embeddings em corpus portugu√™s
- Explorar analogias e similaridades
- Visualizar embeddings com t-SNE

### Exerc√≠cio 3: Avalia√ß√£o de Embeddings
- Implementar m√©tricas de avalia√ß√£o
- Comparar Word2Vec, GloVe e FastText
- Analisar bias nos embeddings

### Exerc√≠cio 4: Redu√ß√£o de Dimensionalidade
- Aplicar PCA em matriz TF-IDF
- Visualizar documentos em espa√ßo reduzido
- Comparar PCA vs t-SNE vs UMAP

### Exerc√≠cio 5: Sistema de Busca
- Implementar busca sem√¢ntica
- Comparar diferentes representa√ß√µes
- Avaliar relev√¢ncia dos resultados

## üö® Armadilhas Comuns

### Problemas com BoW/TF-IDF
- **Esparsidade extrema**: Vocabul√°rio muito grande
- **OOV words**: Palavras n√£o vistas no treino
- **Perda de ordem**: Ignore estrutura sint√°tica
- **Polissemia**: Uma palavra, m√∫ltiplos significados

### Problemas com Embeddings
- **Dados insuficientes**: Precisam de corpus grandes
- **Bias**: Refletem preconceitos dos dados
- **Ambiguidade**: Uma representa√ß√£o por palavra
- **Avalia√ß√£o**: M√©tricas nem sempre refletem qualidade

## üìñ Leituras Complementares

### Papers Fundamentais
- "Efficient Estimation of Word Representations in Vector Space" (Word2Vec)
- "GloVe: Global Vectors for Word Representation"
- "Enriching Word Vectors with Subword Information" (FastText)
- "Distributed Representations of Words and Phrases and their Compositionality"

### Recursos Online
- [Word2Vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)
- [GloVe Project Page](https://nlp.stanford.edu/projects/glove/)
- [FastText Documentation](https://fasttext.cc/)

## üéØ Pr√≥ximos Passos

No **M√≥dulo 5**, vamos aplicar essas representa√ß√µes para classifica√ß√£o de texto usando algoritmos de machine learning.

---

**Dica**: Execute o notebook `04_representacao_texto.ipynb` para experimentar todas as t√©cnicas! 

[‚¨ÖÔ∏è **Voltar**](../README.md) 